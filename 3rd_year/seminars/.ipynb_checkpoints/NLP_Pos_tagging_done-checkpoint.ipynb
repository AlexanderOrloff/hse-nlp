{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Мама мыла раму.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MorphAnalyzer in module pymorphy2.analyzer object:\n",
      "\n",
      "class MorphAnalyzer(builtins.object)\n",
      " |  MorphAnalyzer(path=None, result_type=<class 'pymorphy2.analyzer.Parse'>, units=None, probability_estimator_cls=<class 'pymorphy2.analyzer.SingleTagProbabilityEstimator'>)\n",
      " |  \n",
      " |  Morphological analyzer for Russian language.\n",
      " |  \n",
      " |  For a given word it can find all possible inflectional paradigms\n",
      " |  and thus compute all possible tags and normal forms.\n",
      " |  \n",
      " |  Analyzer uses morphological word features and a lexicon\n",
      " |  (dictionary compiled from XML available at OpenCorpora.org);\n",
      " |  for unknown words heuristic algorithm is used.\n",
      " |  \n",
      " |  Create a :class:`MorphAnalyzer` object::\n",
      " |  \n",
      " |      >>> import pymorphy2\n",
      " |      >>> morph = pymorphy2.MorphAnalyzer()\n",
      " |  \n",
      " |  MorphAnalyzer uses dictionaries from ``pymorphy2-dicts`` package\n",
      " |  (which can be installed via ``pip install pymorphy2-dicts``).\n",
      " |  \n",
      " |  Alternatively (e.g. if you have your own precompiled dictionaries),\n",
      " |  either create ``PYMORPHY2_DICT_PATH`` environment variable\n",
      " |  with a path to dictionaries, or pass ``path`` argument\n",
      " |  to :class:`pymorphy2.MorphAnalyzer` constructor::\n",
      " |  \n",
      " |      >>> morph = pymorphy2.MorphAnalyzer('/path/to/dictionaries') # doctest: +SKIP\n",
      " |  \n",
      " |  By default, methods of this class return parsing results\n",
      " |  as namedtuples :class:`Parse`. This has performance implications\n",
      " |  under CPython, so if you need maximum speed then pass\n",
      " |  ``result_type=None`` to make analyzer return plain unwrapped tuples::\n",
      " |  \n",
      " |      >>> morph = pymorphy2.MorphAnalyzer(result_type=None)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path=None, result_type=<class 'pymorphy2.analyzer.Parse'>, units=None, probability_estimator_cls=<class 'pymorphy2.analyzer.SingleTagProbabilityEstimator'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  cyr2lat(self, tag_or_grammeme)\n",
      " |      Return Latin representation for ``tag_or_grammeme`` string\n",
      " |  \n",
      " |  get_lexeme(self, form)\n",
      " |      Return the lexeme this parse belongs to.\n",
      " |  \n",
      " |  iter_known_word_parses(self, prefix='')\n",
      " |      Return an iterator over parses of dictionary words that starts\n",
      " |      with a given prefix (default empty prefix means \"all words\").\n",
      " |  \n",
      " |  lat2cyr(self, tag_or_grammeme)\n",
      " |      Return Cyrillic representation for ``tag_or_grammeme`` string\n",
      " |  \n",
      " |  normal_forms(self, word)\n",
      " |      Return a list of word normal forms.\n",
      " |  \n",
      " |  parse(self, word)\n",
      " |      Analyze the word and return a list of :class:`pymorphy2.analyzer.Parse`\n",
      " |      namedtuples:\n",
      " |      \n",
      " |          Parse(word, tag, normal_form, para_id, idx, _score)\n",
      " |      \n",
      " |      (or plain tuples if ``result_type=None`` was used in constructor).\n",
      " |  \n",
      " |  tag(self, word)\n",
      " |  \n",
      " |  word_is_known(self, word, strict_ee=False)\n",
      " |      Check if a ``word`` is in the dictionary.\n",
      " |      Pass ``strict_ee=True`` if ``word`` is guaranteed to\n",
      " |      have correct е/ё letters.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Dictionary words are not always correct words;\n",
      " |          the dictionary also contains incorrect forms which\n",
      " |          are commonly used. So for spellchecking tasks this\n",
      " |          method should be used with extra care.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  choose_dictionary_path(path=None) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  TagClass\n",
      " |      :rtype: pymorphy2.tagset.OpencorporaTag\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DEFAULT_UNITS = [[<class 'pymorphy2.units.by_lookup.DictionaryAnalyzer...\n",
      " |  \n",
      " |  ENV_VARIABLE = 'PYMORPHY2_DICT_PATH'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='мама мыла раму.', tag=OpencorporaTag('UNKN'), normal_form='мама мыла раму.', score=1.0, methods_stack=((<UnknAnalyzer>, 'Мама мыла раму.'),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='мама', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='мама', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'мама', 1907, 0),))]\n",
      "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 1),)), Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 1813, 8),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 6),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 9),))]\n",
      "[Parse(word='раму.', tag=OpencorporaTag('UNKN'), normal_form='раму.', score=1.0, methods_stack=((<UnknAnalyzer>, 'раму.'),))]\n"
     ]
    }
   ],
   "source": [
    "for token in text.split():\n",
    "    print(m.parse(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='хейтёр', tag=OpencorporaTag('VERB,impf,tran masc,sing,past,indc'), normal_form='хейтереть', score=0.5555555555555556, methods_stack=((<DictionaryAnalyzer>, 'тёр', 2339, 7), (<UnknownPrefixAnalyzer>, 'хей'))),\n",
       " Parse(word='хейтер', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='хейтер', score=0.22222222222222224, methods_stack=((<FakeDictionary>, 'хейтер', 33, 0), (<KnownSuffixAnalyzer>, 'ейтер'))),\n",
       " Parse(word='хейтер', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='хейтер', score=0.22222222222222224, methods_stack=((<FakeDictionary>, 'хейтер', 33, 3), (<KnownSuffixAnalyzer>, 'ейтер')))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parse('хейтер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='мама', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='мама', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'мама', 1907, 0),))]\n",
      "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 1),)), Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 1813, 8),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 6),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 9),))]\n",
      "[Parse(word='раму', tag=OpencorporaTag('NOUN,inan,masc,Geox sing,datv'), normal_form='рам', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 32, 2),)), Parse(word='раму', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='рама', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 55, 3),))]\n",
      "[Parse(word='.', tag=OpencorporaTag('PNCT'), normal_form='.', score=1.0, methods_stack=((<PunctuationAnalyzer>, '.'),))]\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "for token in simple_word_tokenize(text):\n",
    "    print(m.parse(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = m.parse('знать')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = parse.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on OpencorporaTag in module pymorphy2.tagset object:\n",
      "\n",
      "class OpencorporaTag(builtins.object)\n",
      " |  OpencorporaTag(tag)\n",
      " |  \n",
      " |  Wrapper class for OpenCorpora.org tags.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |      In order to work properly, the class has to be globally\n",
      " |      initialized with actual grammemes (using _init_grammemes method).\n",
      " |  \n",
      " |      Pymorphy2 initializes it when loading a dictionary;\n",
      " |      it may be not a good idea to use this class directly.\n",
      " |      If possible, use ``morph_analyzer.TagClass`` instead.\n",
      " |  \n",
      " |  Example::\n",
      " |  \n",
      " |      >>> from pymorphy2 import MorphAnalyzer\n",
      " |      >>> morph = MorphAnalyzer()\n",
      " |      >>> Tag = morph.TagClass  # get an initialzed Tag class\n",
      " |      >>> tag = Tag('VERB,perf,tran plur,impr,excl')\n",
      " |      >>> tag\n",
      " |      OpencorporaTag('VERB,perf,tran plur,impr,excl')\n",
      " |  \n",
      " |  Tag instances have attributes for accessing grammemes::\n",
      " |  \n",
      " |      >>> print(tag.POS)\n",
      " |      VERB\n",
      " |      >>> print(tag.number)\n",
      " |      plur\n",
      " |      >>> print(tag.case)\n",
      " |      None\n",
      " |  \n",
      " |  Available attributes are: POS, animacy, aspect, case, gender, involvement,\n",
      " |  mood, number, person, tense, transitivity and voice.\n",
      " |  \n",
      " |  You may check if a grammeme is in tag or if all grammemes\n",
      " |  from a given set are in tag::\n",
      " |  \n",
      " |      >>> 'perf' in tag\n",
      " |      True\n",
      " |      >>> 'nomn' in tag\n",
      " |      False\n",
      " |      >>> 'Geox' in tag\n",
      " |      False\n",
      " |      >>> set(['VERB', 'perf']) in tag\n",
      " |      True\n",
      " |      >>> set(['VERB', 'perf', 'sing']) in tag\n",
      " |      False\n",
      " |  \n",
      " |  In order to fight typos, for unknown grammemes an exception is raised::\n",
      " |  \n",
      " |      >>> 'foobar' in tag\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      ValueError: Grammeme is unknown: foobar\n",
      " |      >>> set(['NOUN', 'foo', 'bar']) in tag\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      ValueError: Grammemes are unknown: {'bar', 'foo'}\n",
      " |  \n",
      " |  This also works for attributes::\n",
      " |  \n",
      " |      >>> tag.POS == 'plur'\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      ValueError: 'plur' is not a valid grammeme for this attribute. Valid grammemes: ...\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  POS\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  __contains__(self, grammeme)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, tag)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  animacy\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  aspect\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  case\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  gender\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  involvement\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  is_productive(self)\n",
      " |  \n",
      " |  mood\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  number\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  numeral_agreement_grammemes(self, num)\n",
      " |  \n",
      " |  person\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  tense\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  transitivity\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  updated_grammemes(self, required)\n",
      " |      Return a new set of grammemes with ``required`` grammemes added\n",
      " |      and incompatible grammemes removed.\n",
      " |  \n",
      " |  voice\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  add_grammemes_to_known(lat, cyr) from builtins.type\n",
      " |  \n",
      " |  cyr2lat(tag_or_grammeme) from builtins.type\n",
      " |      Return Latin representation for ``tag_or_grammeme`` string\n",
      " |  \n",
      " |  fix_rare_cases(grammemes) from builtins.type\n",
      " |      Replace rare cases (loc2/voct/...) with common ones (loct/nomn/...).\n",
      " |  \n",
      " |  grammeme_is_known(grammeme) from builtins.type\n",
      " |  \n",
      " |  lat2cyr(tag_or_grammeme) from builtins.type\n",
      " |      Return Cyrillic representation for ``tag_or_grammeme`` string\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cyr_repr\n",
      " |      Cyrillic representation of this tag\n",
      " |  \n",
      " |  grammemes\n",
      " |      A frozenset with grammemes for this tag.\n",
      " |  \n",
      " |  grammemes_cyr\n",
      " |      A frozenset with Cyrillic grammemes for this tag.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ANIMACY = frozenset({'anim', 'inan'})\n",
      " |  \n",
      " |  ASPECTS = frozenset({'impf', 'perf'})\n",
      " |  \n",
      " |  CASES = frozenset({'ablt', 'acc2', 'accs', 'datv', 'gen1', 'gen2', ......\n",
      " |  \n",
      " |  FORMAT = 'opencorpora-int'\n",
      " |  \n",
      " |  GENDERS = frozenset({'femn', 'masc', 'neut'})\n",
      " |  \n",
      " |  INVOLVEMENT = frozenset({'excl', 'incl'})\n",
      " |  \n",
      " |  KNOWN_GRAMMEMES = {'1per', '2per', '3per', 'ADJF', 'ADJS', 'ADVB', ......\n",
      " |  \n",
      " |  MOODS = frozenset({'impr', 'indc'})\n",
      " |  \n",
      " |  NUMBERS = frozenset({'plur', 'sing'})\n",
      " |  \n",
      " |  PARTS_OF_SPEECH = frozenset({'ADJF', 'ADJS', 'ADVB', 'COMP', 'CONJ', '...\n",
      " |  \n",
      " |  PERSONS = frozenset({'1per', '2per', '3per'})\n",
      " |  \n",
      " |  RARE_CASES = {'acc1': 'accs', 'acc2': 'accs', 'gen1': 'gent', 'gen2': ...\n",
      " |  \n",
      " |  TENSES = frozenset({'futr', 'past', 'pres'})\n",
      " |  \n",
      " |  TRANSITIVITY = frozenset({'intr', 'tran'})\n",
      " |  \n",
      " |  VOICES = frozenset({'actv', 'pssv'})\n",
      " |  \n",
      " |  typed_grammemes = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INFN', 'impf', 'tran']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tag.grammemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Форматы корпусов\n",
    "\n",
    "Для простых задач часто подходит формат корпусов NLTK ([tagged_words](https://www.nltk.org/book/ch05.html)/tagged_sents).\n",
    "Корпуса с разметкой зависимостей и подробной морфологической разметкой часто хранятся в формате [CoNLL](https://universaldependencies.org/format.html).\n",
    "Его удобно читать библиотекой `conllu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте скачаем часть корпуса Universal Dependencies для русского - СинТагРус\n",
    "import requests\n",
    "r = requests.get(\"https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-train.conllu\")\n",
    "train_corpus = r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = 2003Anketa.xml_1\n",
      "# text = Анкета.\n",
      "1\tАнкета\tанкета\tNOUN\t_\tAnimacy=Inan|Case=Nom|Gender=Fem|Number=Sing\t0\troot\t0:root\tSpaceAfter=No\n",
      "2\t.\t.\tPUNCT\t_\t_\t1\tpunct\t1:punct\t_\n",
      "\n",
      "# sent_id = 2003Anketa.xml_2\n",
      "# text = Начальник областного управления связи Семен Еремеевич был человек простой, приходил на работу всегда вовремя, здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \"Муха\".\n",
      "1\tНачальник\tначальник\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t8\t\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus.decode('utf-8')[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = conllu.parse(train_corpus.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48814"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('id', 1), ('form', 'Начальник'), ('lemma', 'начальник'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 8), ('deprel', 'nsubj'), ('deps', [('nsubj', 8)]), ('misc', None)])\n",
      "OrderedDict([('id', 2), ('form', 'областного'), ('lemma', 'областной'), ('upostag', 'ADJ'), ('xpostag', None), ('feats', OrderedDict([('Case', 'Gen'), ('Degree', 'Pos'), ('Gender', 'Neut'), ('Number', 'Sing')])), ('head', 3), ('deprel', 'amod'), ('deps', [('amod', 3)]), ('misc', None)])\n",
      "OrderedDict([('id', 3), ('form', 'управления'), ('lemma', 'управление'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Inan'), ('Case', 'Gen'), ('Gender', 'Neut'), ('Number', 'Sing')])), ('head', 1), ('deprel', 'nmod'), ('deps', [('nmod', 1)]), ('misc', None)])\n",
      "OrderedDict([('id', 4), ('form', 'связи'), ('lemma', 'связь'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Inan'), ('Case', 'Gen'), ('Gender', 'Fem'), ('Number', 'Sing')])), ('head', 3), ('deprel', 'nmod'), ('deps', [('nmod', 3)]), ('misc', None)])\n",
      "OrderedDict([('id', 5), ('form', 'Семен'), ('lemma', 'Семен'), ('upostag', 'PROPN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 1), ('deprel', 'appos'), ('deps', [('appos', 1)]), ('misc', None)])\n",
      "OrderedDict([('id', 6), ('form', 'Еремеевич'), ('lemma', 'Еремеевич'), ('upostag', 'PROPN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 5), ('deprel', 'flat:name'), ('deps', [('flat:name', 5)]), ('misc', None)])\n",
      "OrderedDict([('id', 7), ('form', 'был'), ('lemma', 'быть'), ('upostag', 'AUX'), ('xpostag', None), ('feats', OrderedDict([('Aspect', 'Imp'), ('Gender', 'Masc'), ('Mood', 'Ind'), ('Number', 'Sing'), ('Tense', 'Past'), ('VerbForm', 'Fin'), ('Voice', 'Act')])), ('head', 8), ('deprel', 'cop'), ('deps', [('cop', 8)]), ('misc', None)])\n",
      "OrderedDict([('id', 8), ('form', 'человек'), ('lemma', 'человек'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 0), ('deprel', 'root'), ('deps', '0:root'), ('misc', None)])\n",
      "OrderedDict([('id', 9), ('form', 'простой'), ('lemma', 'простой'), ('upostag', 'ADJ'), ('xpostag', None), ('feats', OrderedDict([('Case', 'Nom'), ('Degree', 'Pos'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 8), ('deprel', 'amod'), ('deps', [('amod', 8)]), ('misc', OrderedDict([('SpaceAfter', 'No')]))])\n",
      "OrderedDict([('id', 10), ('form', ','), ('lemma', ','), ('upostag', 'PUNCT'), ('xpostag', None), ('feats', None), ('head', 11), ('deprel', 'punct'), ('deps', [('punct', 11)]), ('misc', None)])\n"
     ]
    }
   ],
   "source": [
    "# Каждое предложение - список токенов\n",
    "# А токен представлен словарем значений\n",
    "for t in sentences[1][:10]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем корпус\n",
    "Давайте создадим список пар (tuple) слово (form) + тег (upostag) для каждого предложения. Получим структуру, аналогичную `tagged_sents` из `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sents = []\n",
    "for s in sentences:\n",
    "    pos_sents.append([])\n",
    "    for t in s:\n",
    "        pos_sents[-1].append((t['form'], t['upostag']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48814"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Однако', 'ADV'),\n",
       " ('стиль', 'NOUN'),\n",
       " ('работы', 'NOUN'),\n",
       " ('Семена', 'PROPN'),\n",
       " ('Еремеевича', 'PROPN'),\n",
       " ('заключался', 'VERB'),\n",
       " ('в', 'ADP'),\n",
       " ('том', 'PRON'),\n",
       " (',', 'PUNCT'),\n",
       " ('чтобы', 'SCONJ'),\n",
       " ('принимать', 'VERB'),\n",
       " ('всех', 'DET'),\n",
       " ('желающих', 'VERB'),\n",
       " ('и', 'CCONJ'),\n",
       " ('лично', 'ADV'),\n",
       " ('вникать', 'VERB'),\n",
       " ('в', 'ADP'),\n",
       " ('дело', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sents[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простейший POS-теггер\n",
    "Для каждого слова определим какими тегами оно бывает отмечено в обучающем корпусе, а для предсказания выбираем наиболее частотный тег:\n",
    "$$\n",
    "tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(tag_i \\mid w).\n",
    "$$\n",
    "NB! Если какое-то слово в обучающем корпусе мы не встретили, то тег на нем никаким образом уже поставить не сможем.\n",
    "\n",
    "Для этого нам нужно:\n",
    "* посчитать все частоты слово + тег:\n",
    "$$count(t_i, w_i)$$\n",
    "* частоту слова легко будет вывести из этих частот:\n",
    "$$count(w) = \\sum_{i=0...N}count(t_i, w_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать структуру `Counter` для хранения частот:\n",
    "`Counter[k] = 0` по умолчанию, поэтому можно сразу прибавлять:\n",
    "`Counter[k] += 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализатор для получения распределения вероятностей из частот\n",
    "# p(w) = count(w) / sum([count(w_i) for w_i in counter])\n",
    "def normalize(counter):\n",
    "    sum_ = sum(counter.values())\n",
    "    for token in counter:\n",
    "        counter[token] /= sum_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удобно использовать defaultdict, который заранее задает дефолтное значение для каждого ключа\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистическая модель\n",
    "# Здесь будут храниться слова и вероятности их тегов, например:\n",
    "# { 'знать': Counter({'NOUN': 0.3, 'VERB': 0.7})}\n",
    "class WordTagModel:\n",
    "    def __init__(self, tagged_sents):\n",
    "        self.model = defaultdict(Counter)\n",
    "        # Для каждого слова добавим Counter с частотами тегов\n",
    "        for sent in tagged_sents:\n",
    "            for w, t in sent:\n",
    "                self.model[w][t] += 1\n",
    "        # Не забудем нормализовать частоты, чтобы получить вероятности\n",
    "        for word in self.model:\n",
    "            normalize(self.model[word])\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        # специальная функция, чтобы получать значение по ключу (=слову)\n",
    "        return self.model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'Анкета': Counter({'NOUN': 1.0}),\n",
       "             '.': Counter({'PUNCT': 1.0}),\n",
       "             'Начальник': Counter({'NOUN': 1.0}),\n",
       "             'областного': Counter({'ADJ': 1.0}),\n",
       "             'управления': Counter({'NOUN': 1.0}),\n",
       "             'связи': Counter({'NOUN': 1.0}),\n",
       "             'Семен': Counter({'PROPN': 1.0}),\n",
       "             'Еремеевич': Counter({'PROPN': 1.0}),\n",
       "             'был': Counter({'AUX': 1.0}),\n",
       "             'человек': Counter({'NOUN': 1.0}),\n",
       "             'простой': Counter({'ADJ': 1.0}),\n",
       "             ',': Counter({'PUNCT': 1.0}),\n",
       "             'приходил': Counter({'VERB': 1.0}),\n",
       "             'на': Counter({'ADP': 1.0}),\n",
       "             'работу': Counter({'NOUN': 1.0}),\n",
       "             'всегда': Counter({'ADV': 1.0}),\n",
       "             'вовремя': Counter({'ADV': 1.0}),\n",
       "             'здоровался': Counter({'VERB': 1.0}),\n",
       "             'с': Counter({'ADP': 1.0}),\n",
       "             'секретаршей': Counter({'NOUN': 1.0}),\n",
       "             'за': Counter({'ADP': 1.0}),\n",
       "             'руку': Counter({'NOUN': 1.0}),\n",
       "             'и': Counter({'CCONJ': 0.6666666666666666,\n",
       "                      'PART': 0.3333333333333333}),\n",
       "             'иногда': Counter({'ADV': 1.0}),\n",
       "             'даже': Counter({'PART': 1.0}),\n",
       "             'писал': Counter({'VERB': 1.0}),\n",
       "             'в': Counter({'ADP': 1.0}),\n",
       "             'стенгазету': Counter({'NOUN': 1.0}),\n",
       "             'заметки': Counter({'NOUN': 1.0}),\n",
       "             'под': Counter({'ADP': 1.0}),\n",
       "             'псевдонимом': Counter({'NOUN': 1.0}),\n",
       "             '\"': Counter({'PUNCT': 1.0}),\n",
       "             'Муха': Counter({'NOUN': 1.0}),\n",
       "             'В': Counter({'ADP': 1.0}),\n",
       "             'приемной': Counter({'NOUN': 1.0}),\n",
       "             'его': Counter({'PRON': 1.0}),\n",
       "             'утра': Counter({'NOUN': 1.0}),\n",
       "             'ожидали': Counter({'VERB': 1.0}),\n",
       "             'посетители': Counter({'NOUN': 1.0}),\n",
       "             '-': Counter({'PUNCT': 1.0}),\n",
       "             'кое-кто': Counter({'PRON': 1.0}),\n",
       "             'важными': Counter({'ADJ': 1.0}),\n",
       "             'делами': Counter({'NOUN': 1.0}),\n",
       "             'а': Counter({'CCONJ': 1.0}),\n",
       "             'такими': Counter({'DET': 1.0}),\n",
       "             'которые': Counter({'PRON': 1.0}),\n",
       "             'легко': Counter({'ADV': 1.0}),\n",
       "             'можно': Counter({'ADV': 1.0}),\n",
       "             'было': Counter({'AUX': 1.0}),\n",
       "             'решить': Counter({'VERB': 1.0}),\n",
       "             'нижестоящих': Counter({'ADJ': 1.0}),\n",
       "             'инстанциях': Counter({'NOUN': 1.0}),\n",
       "             'не': Counter({'PART': 1.0}),\n",
       "             'затрудняя': Counter({'VERB': 1.0}),\n",
       "             'Семена': Counter({'PROPN': 1.0}),\n",
       "             'Еремеевича': Counter({'PROPN': 1.0}),\n",
       "             'Однако': Counter({'ADV': 1.0}),\n",
       "             'стиль': Counter({'NOUN': 1.0}),\n",
       "             'работы': Counter({'NOUN': 1.0}),\n",
       "             'заключался': Counter({'VERB': 1.0}),\n",
       "             'том': Counter({'PRON': 1.0}),\n",
       "             'чтобы': Counter({'SCONJ': 1.0}),\n",
       "             'принимать': Counter({'VERB': 1.0}),\n",
       "             'всех': Counter({'DET': 1.0}),\n",
       "             'желающих': Counter({'VERB': 1.0}),\n",
       "             'лично': Counter({'ADV': 1.0}),\n",
       "             'вникать': Counter({'VERB': 1.0}),\n",
       "             'дело': Counter({'NOUN': 1.0}),\n",
       "             'Приемная': Counter({'NOUN': 1.0}),\n",
       "             'была': Counter({'AUX': 1.0}),\n",
       "             'обставлена': Counter({'VERB': 1.0}),\n",
       "             'просто': Counter({'ADV': 1.0}),\n",
       "             'но': Counter({'CCONJ': 1.0}),\n",
       "             'по-деловому': Counter({'ADV': 1.0})})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WordTagModel(pos_sents[:5])\n",
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем теггер\n",
    "class SimplePOSTagger:\n",
    "    # Инициализировать теггер будем моделью word_tag_model, реализация которой выше\n",
    "    def __init__(self, word_tag_model):\n",
    "        self.wts = word_tag_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        # sent - предложение - список словоформ\n",
    "        tags = []\n",
    "        for word in sent:\n",
    "            c = self.wts[word]\n",
    "            if c:\n",
    "                tags.append(c.most_common(1)[0][0])\n",
    "            else:\n",
    "                tags.append(\"UNK\")\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Мама', 'NOUN'), ('мыла', 'NOUN'), ('раму', 'UNK')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем\n",
    "wtm = WordTagModel(pos_sents)\n",
    "postagger = SimplePOSTagger(wtm)\n",
    "postagger.tag(\"Мама мыла раму\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос!**\n",
    "Как можно улучшить наш простой теггер?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM tagger\n",
    "Для каждого слова будем выбирать наиболее вероятный тег, учитывая общую вероятность самого тега. Тут можно будет добавить сглаживание для слов, которых в модели не было.\n",
    "$$\n",
    "     tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(w \\mid tag_i)P(tag_i)\n",
    "$$\n",
    "Для этого нам понадобятся новые классы:\n",
    "\n",
    "**EmissionModel**, хранящий для каждого тега вероятности быть присвоенным тому или иному слову:\n",
    "\n",
    "```\n",
    "defaultdict(\n",
    "  {\n",
    "    'tag_1': Counter({'word_1': 0.3, 'word_2': 0.7}), \n",
    "    'tag_2': Counter({'word_1': 0.6, 'word_3': 0.3 ...})\n",
    "  }\n",
    ")\n",
    "```\n",
    "\n",
    "**TransitionModel**, отвечающий за вероятность $P(tag_i)$:\n",
    "\n",
    "```\n",
    "Counter(\n",
    "  {\n",
    "    'tag_1': 0.1, \n",
    "    'tag_2': 0.33,\n",
    "    ...\n",
    "  }\n",
    ")\n",
    "```\n",
    "\n",
    "**UnigramPOSTagger**, сопоставляющий последовательности слов последовательность тегов, будет содержать `EmissionModel` и `TransitionModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionModel:\n",
    "    def __init__(self, tagged_sents):\n",
    "        self.model = defaultdict(Counter)\n",
    "        for sent in tagged_sents:\n",
    "            for w, t in sent:\n",
    "                self.model[t][w] += 1\n",
    "        self.add_unk_token()\n",
    "        for tag in self.model:\n",
    "            normalize(self.model[tag])\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        # Для каждого тега добавим одинаковую вероятность быть приписанным любому слову, которого нет в модели\n",
    "        for tag in self.model:\n",
    "            self.model[tag]['UNK'] = 0.01\n",
    "        \n",
    "    def tags(self):\n",
    "        # Добавим возможность возвращать все теги, которые есть в модели\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        # Все слова для данного тега\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, word, tag):\n",
    "        # Самое интересное - вероятность P(tag|word)\n",
    "        if word not in self[tag]:\n",
    "            return self[tag]['UNK']\n",
    "        return self[tag][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel:\n",
    "    def __init__(self, tagged_sents):\n",
    "        # Это модель будет хранить вероятности P(tag): Counter({'tag_1': 0.34, 'tag_2': 0.1 ...})\n",
    "        # Поэтому для неё нужны только пооследовательности тегов\n",
    "        self.model = Counter()\n",
    "        for sent in tagged_sents:\n",
    "            for _, t in sent:\n",
    "                self.model[t] += 1\n",
    "        normalize(self.model)\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, tag):\n",
    "        return self.model[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramPOSTagger:\n",
    "    def __init__(self, emission_model, transition_model):\n",
    "        self.em = emission_model\n",
    "        self.tm = transition_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        # Для списка слов возвращаем список пар (слово, тег)\n",
    "        # Для каждого слова проходимся по всем тегам\n",
    "        # И выбираем максимум по формуле\n",
    "        tags = []\n",
    "        for word in sent:\n",
    "            max_tag, max_prob = \"UNK\", 0.\n",
    "            for tag in self.tm.tags():\n",
    "                score = self.em(word, tag) * self.tm(tag)\n",
    "                if score > max_prob:\n",
    "                    max_prob = score\n",
    "                    max_tag = tag\n",
    "            tags.append(max_tag)\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем!\n",
    "em = EmissionModel(pos_sents)\n",
    "tm = TransitionModel(pos_sents)\n",
    "unigram_postagger = UnigramPOSTagger(em, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Мама', 'NOUN'), ('мыла', 'NOUN'), ('раму', 'NOUN')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_postagger.tag('Мама мыла раму'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-based tagger\n",
    "\n",
    "Довольно долго обучается в Google Colab:\n",
    "https://colab.research.google.com/drive/15KS4NtNKjzokzWYjIjaQ58MWWl4luZUr?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
