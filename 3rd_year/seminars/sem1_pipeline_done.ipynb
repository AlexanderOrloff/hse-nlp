{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia pymorphy2 textdistance -q"
      ],
      "metadata": {
        "id": "rKc92Eevlnyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5dbd84-1d84-42c2-8e3b-7b3b4d5d658f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDFUuSxGaic"
      },
      "source": [
        "# Семинар 1. Базовый пайплайн работы с текстом. Создания простого спеллчекера с помощью NLTK, векторов и рассстояния Левенштейна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2GsLxb7TyKc"
      },
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1Dh0iSF2AM-ApFQoMLigmDDBaNO978EAi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5uuHtVSdY_C"
      },
      "source": [
        "## 1. Сбор данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh0CDmMmiScF"
      },
      "source": [
        "Под основные задачи было придумано огромное количество датасетов: [тональность](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset), [NER](https://github.com/juand-r/entity-recognition-datasets), [QA](https://rajpurkar.github.io/SQuAD-explorer/). Но... большинство данных на английском:)\n",
        "\n",
        "Где нам взять данные для того, чтобы....\n",
        "1. Обучить нейросеть определять тональность высказывания (проще говоря, является ли текст положительным/хвалебным, негативным или нейтральным)\n",
        "2. Создать программу, которая будет определять, написан ли текст в формальном стиле или нет\n",
        "3. Обучить нейросеть определять хейтспич?\n",
        "4. Создать  функцию, которая убирает из текста стоп-слова?\n",
        "5. Создать спеллчекер?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PF9wrJlermY"
      },
      "source": [
        "### 1.1 Готовые корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCK9BaXbfOuN",
        "collapsed": true
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "nltk.download('all-corpora')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QhD_tsxfRTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8e8254-e24d-46a2-c7cf-8c5216fdf3e4"
      },
      "source": [
        "from nltk.book import text1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8rIGbJqfC8a",
        "outputId": "b8625c15-6d10-460a-898f-47d682c63e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260819"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text1[:100]"
      ],
      "metadata": {
        "id": "fHUUoGCTfmCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance('red')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFJBRBQvdzXV",
        "outputId": "4907fe3a-cdfe-4f04-c813-be32ef15c8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 25 of 40 matches:\n",
            "t did those aboriginal whalemen , the Red - Men , first sally out in canoes to \n",
            " there . Further on , from the bright red windows of the \" Sword - Fish Inn ,\" \n",
            " Euroclydon ! says old Dives , in his red silken wrapper --( he had a redder on\n",
            "w Bedford , they bloom like their own red roses . But roses only bloom in summe\n",
            "ed the chapel with a handsome pair of red worsted man - ropes for this ladder ,\n",
            "of how this island was settled by the red - men . Thus goes the legend . In old\n",
            "n the porch of the inn , under a dull red lamp swinging there , that looked muc\n",
            " Quohog ! spring , thou chap with the red whiskers ; spring there , Scotch - ca\n",
            "ists the last remnant of a village of red men , which has long supplied the nei\n",
            "him from his mood . For , as when the red - cheeked , dancing girls , April and\n",
            "e benignity of age ; though among the Red Men of America the giving of the whit\n",
            "hen , with tornado brow , and eyes of red murder , and foam - glued lips , Ahab\n",
            "ir beaches to wild barbarians , whose red painted faces flash from out their pe\n",
            "down on the windlass ; his face fiery red , his eyes bloodshot , and wiping the\n",
            "ently floats , openly toying with his red - cheeked Cleopatra , ripening his ap\n",
            "ain , with some tatters of Radney ' s red woollen shirt , caught in the teeth t\n",
            "en ranging up for another fling . The red tide now poured from all sides of the\n",
            "at they all glowed to each other like red men . And all the while , jet after j\n",
            " At last , gush after gush of clotted red gore , as if it had been the purple l\n",
            " as if it had been the purple lees of red wine , shot into the frighted air ; a\n",
            " like hungry dogs round a table where red meat is being carved , ready to bolt \n",
            "ught we were offering up ten thousand red oxen to the sea gods . In the first p\n",
            "nd line - of - battle - ship loads of red - haired devils . What d ' ye say , T\n",
            "other passage up the Persian Gulf and Red Sea , such a supposition would involv\n",
            "ad of sparkling water , he now spouts red blood . \" That drove the spigot out o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am0LidP5hKbg"
      },
      "source": [
        "from nltk.corpus import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?nltk.corpus"
      ],
      "metadata": {
        "id": "UDdp0q21f9Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.corpus.names.words('female.txt')"
      ],
      "metadata": {
        "id": "T1c6cqJqcsvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну и конечно, наше любимое, это стоп-слова."
      ],
      "metadata": {
        "id": "H7JwdcXHFoPP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLf269Qxg7JH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62873c70-d82c-4c08-df8d-6087875b68a3",
        "collapsed": true
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "russian_stopwords[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bDoyyV-h-7R"
      },
      "source": [
        "Существует и множество других встроенных библиотек с текстами (к примеру sklearn). Корпуса есть и на русском. Однако не редко встроенных текстов не достаточно и приходится пользоваться парсингом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSID2k6_eHlp"
      },
      "source": [
        "### 1.2 Парсинг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFKhqNl4GT57"
      },
      "source": [
        "import urllib.request\n",
        "import re\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GgNvmcKeesS"
      },
      "source": [
        "def parcer(url):\n",
        "   page = urllib.request.urlopen(url)\n",
        "   text = page.read().decode('utf-8')\n",
        "   return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESeIXBBBkxTU"
      },
      "source": [
        "url = 'https://ru.wikipedia.org/wiki/%D0%9B%D0%B8%D0%BD%D0%B3%D0%B2%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zu1Lqluk6ji"
      },
      "source": [
        "text = parcer(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-uyTOclk_Wd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c0cc0647-54e5-425d-e86e-66c82e425ea3"
      },
      "source": [
        "text[:1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"ru\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\">\\n<title>Лингвистика — Википедия</title>\\n<script>(function(){var className=\"client-js\";var cookie=document.cookie.match(/(?:^|; )ruwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split(\\'%2C\\').forEach(function(pref){className=className.replace(new RegExp(\\'(^| )\\'+pref.replace(/-clientpref-\\\\w+$|[^\\\\w-]+/g,\\'\\')+\\'-clientpref-\\\\\\\\w+( |$)\\'),\\'$1\\'+pref+\\'$2\\');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\",\\\\t.\",\"\\xa0\\\\t,\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"январь\",\"февраль\",\"март\",\"апрель\",\"май\",\"июнь\",\"июль\",\"август\",\"сентябрь\",\"октябрь\",\"ноябрь\",\"декабрь\"],\"wgRequestId\":\"5bc47c73-abbf-4d0c-a80a-e6aa1fdb69ed\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Лингвистика\",\"wgTitle\":\"Лингвистика\",\"wgCurRevisionId\":139826574,\"wgRevisionId\":139826574,\"w'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOdYDx9olRMG"
      },
      "source": [
        "#### Тренировка на регулярки!\n",
        "Как можно быстро убрать теги?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la4JobEip7D9"
      },
      "source": [
        "import re\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  text_f = re.search('/table>(.*)', raw_html, flags=re.DOTALL)\n",
        "  cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', text_f.group(1))\n",
        "  cleaner = re.sub('\\n', ' ', cleantext)\n",
        "  cleaner = re.sub(r'([а-я])(\\d{1,2})', r'\\1', cleaner)\n",
        "  return cleaner\n",
        "\n",
        "clean_text = cleanhtml(parcer(url))\n",
        "clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Автоматизация: BeautifulSoup"
      ],
      "metadata": {
        "id": "_-lONBKWlBJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "text = parcer(url)\n",
        "soup = BeautifulSoup(text, 'html.parser')\n",
        "print(soup.text.replace('\\n\\n\\n', ' ')[862:1200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ8XuIjMlAmM",
        "outputId": "94ad117b-0a7a-4c69-b43a-34cea19f1b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лингви́стика (от лат. lingua «язык»), языкозна́ние, языкове́дение — наука, изучающая язык[1][2].\n",
            "Это наука о естественном человеческом языке вообще и обо всех языках мира как его индивидуализированных представителях.\n",
            "В широком смысле слова лингвистика подразделяется на научную и практическую. Чаще всего под лингвистикой подразумевается \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prfaMIwxWr7_"
      },
      "source": [
        "Важно помнить, что каждый отдельный сайт имеет свою структуру, а следовательно, требует отдельных методов обработки. Поэтому, для сохранения нервов и времени, стоит брать сразу такие сайты, в которых есть достаточно данных для дальнейшей работы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JurjdwY849G7"
      },
      "source": [
        "#### Автоматизация: Wikipedia\n",
        "Многие крупные страницы уже имеют решения по автоматической обработке данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJviRSt92Xpi"
      },
      "source": [
        "import wikipedia as wpd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Ta0qnC5TKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9cdcf23-6b2c-445a-fe2e-40dcfdef1867"
      },
      "source": [
        "li = wpd.page(\"Linguistics\")\n",
        "print(li.title)\n",
        "print(li.url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linguistics\n",
            "https://en.wikipedia.org/wiki/Linguistics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctvY0K1fXYop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c476d38d-e973-41ce-e6e5-b10c2b835d59",
        "collapsed": true
      },
      "source": [
        "wpd.set_lang(\"ru\")\n",
        "ly = wpd.page(\"Лингвистика\")\n",
        "\n",
        "ly.links[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Gemeinsame Normdatei',\n",
              " 'Акустика',\n",
              " 'Акустика речи',\n",
              " 'Анкетирование в лингвистике',\n",
              " 'Антропологическая лингвистика',\n",
              " 'Антропонимика',\n",
              " 'Арго',\n",
              " 'Ареалогия',\n",
              " 'Ареальная лингвистика',\n",
              " 'Артикуляционная фонетика']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pz1aLB05j5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10ed0e3-632d-46f2-874f-644fc3b604e1"
      },
      "source": [
        "clean_text = ly.content\n",
        "print(clean_text[:470])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лингви́стика (от лат. lingua «язык»), языкозна́ние, языкове́дение — наука, изучающая язык.\n",
            "Это наука о естественном человеческом языке вообще и обо всех языках мира как его индивидуализированных представителях.\n",
            "В широком смысле слова лингвистика подразделяется на научную и практическую. Чаще всего под лингвистикой подразумевается именно научная лингвистика; профессионально ей занимаются учёные-лингвисты. Лингвистика связана с семиотикой как наукой о знаковых система\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVA9OYwC5hp3"
      },
      "source": [
        "__Вопрос на подумать:__\n",
        "\n",
        "> Как из имеющихся команд можно быстро собрать большое количество, похожих по теме статей?\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0r-QEbGbEcW",
        "collapsed": true
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_similar_texts(lang, page_name, num=10):\n",
        "    wiki_content = []\n",
        "    wpd.set_lang(lang)\n",
        "    pbar = tqdm(total=num)\n",
        "\n",
        "    page = wpd.page(page_name)\n",
        "    wiki_content.append(\"%s\\n%s\" % (page.title, page.content.replace('=', '')))\n",
        "    pbar.update(1)\n",
        "\n",
        "    i = 0\n",
        "    while (len(wiki_content) != num) and (i < len(wiki_content)):\n",
        "        page_name = page.links[i]\n",
        "        try:\n",
        "            page = wpd.page(page_name)\n",
        "            wiki_content.append(\"%s\\n%s\" % (page.title, page.content.replace('=', '')))\n",
        "            pbar.update(1)\n",
        "        except wpd.exceptions.WikipediaException:\n",
        "            print(\"Skip %s\" % page_name)\n",
        "        i += 1\n",
        "\n",
        "    pbar.close()\n",
        "    return wiki_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_texts = get_similar_texts('ru', 'Лингвистика')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eV12ualRGPW",
        "outputId": "126a57e8-40cd-458b-e2b9-980b9d264405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:08<00:00,  1.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similar_texts[:2]"
      ],
      "metadata": {
        "id": "uq8-Hb7VIi1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaUjks2Ywuht"
      },
      "source": [
        "На семинарах мы, как правило, будем работать на относительно небольшых объёмах данных -- так что сейчас мы не будем скачивать большой текст для нашего спеллчекера, а поработаем на относительно небольшом объёме данных -- на нашем тексте про лингвистику."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xiOVnQ59fH"
      },
      "source": [
        "## 2. Препроцессинг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHRHtOht6GQL"
      },
      "source": [
        "Существуют различные виды препроцессинга, подходящие для разных видов задач:\n",
        "\n",
        "1. Токенизация\n",
        "2. Нормализация/лемматизация\n",
        "3. POS-tagging\n",
        "4. Чанкинг\n",
        "5. Словари и множества для хранения данных\n",
        "\n",
        "А ещё синтаксические парсеры, деревья, поиск именованных сущностей и тд."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd4gb6xcCAqE"
      },
      "source": [
        "### 2.1 Токенизация"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Какой может быть токенизация? Предложите свой вариант токенизации в ячейке ниже."
      ],
      "metadata": {
        "id": "YBZe6FwgTJkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'Из-под стола выглянул рыжий кот, я удивился'"
      ],
      "metadata": {
        "id": "rcjwcz4NNPrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "print(punctuation)"
      ],
      "metadata": {
        "id": "CrQzUuwShOtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00dd1d2-b634-4b6b-eb80-4292b968beb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation += '«»—'"
      ],
      "metadata": {
        "id": "MDjF2ZpBcKnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFUPrXY78qTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6702174-bc0e-4f17-f478-ea2c941f2fbc"
      },
      "source": [
        "split_sent = [word.strip(punctuation) for word in sent.split()]\n",
        "split_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Из-под', 'стола', 'выглянул', 'рыжий', 'кот', 'я', 'удивился']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обычно используется NLTK для токенизации"
      ],
      "metadata": {
        "id": "wrS79G1ENX2Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YOOfRGD8whJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c131494c-3feb-4a08-e050-926a7da6e5dd"
      },
      "source": [
        "tokens = nltk.word_tokenize(sent)\n",
        "tokens[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Из-под', 'стола', 'выглянул', 'рыжий', 'кот', ',', 'я', 'удивился']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.wordpunct_tokenize(sent)\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MULsYl1gIOB",
        "outputId": "89ee42fa-5c3e-4019-8868-d4464ca58760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Из', '-', 'под', 'стола', 'выглянул', 'рыжий', 'кот', ',', 'я', 'удивился']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Лемматизация"
      ],
      "metadata": {
        "id": "3yuVfomYJUuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разным языкам - разные лемматизаторы. Да и сами лемматизаторы разные:\n",
        "- pymorphy: большой словарь, которые подбирает форму слова на основе высчитанной статистики.\n",
        "    \n",
        "    \\+ работает быстро\n",
        "\n",
        "    \\- не всегда верно, так как не опирается на контекст\n",
        "- mystem: тоже основано на словарях, но при этом умеет в контекст и ещё может анализировать неизвестные слова на основе известной ей морфологии\n",
        "\n",
        "    \\+ умеет снимать однозначность\n",
        "\n",
        "    \\- работает медленнеее (хотя на больших текстах может и быстрее)\n",
        "\n",
        "И ещё много-много-много, но эти __база__."
      ],
      "metadata": {
        "id": "dt_ScOpeNCqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer(lang='ru')"
      ],
      "metadata": {
        "id": "JFjkj0sLJUM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'Для безе надо взбить белки'\n",
        "tokens_pymorphy = nltk.word_tokenize(sent)\n",
        "tokens_pymorphy[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTDj-6IEPTea",
        "outputId": "be80899e-777e-4d3a-bc5c-1c8f8fb7e9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Для', 'безе', 'надо', 'взбить', 'белки']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens_pymorphy[:10]:\n",
        "    print(token, morph.parse(token)[0].normal_form, sep='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZUtHMg8JeYb",
        "outputId": "cf27fa0f-1823-4517-a879-6c1636b5737d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Для\tдля\n",
            "безе\tбезе\n",
            "надо\tнадо\n",
            "взбить\tвзбить\n",
            "белки\tбелка\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem = Mystem()"
      ],
      "metadata": {
        "id": "-gbnYIZiINt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0759b044-8e96-409f-fba6-d8b89b1b6c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = mystem.lemmatize(sent)\n",
        "print(sent)\n",
        "print(''.join(lemmas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJLeQtRqM6aq",
        "outputId": "c6e7b456-7a71-4034-d223-b12045c02e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Для безе надо взбить белки\n",
            "для безе надо взбивать белок\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd8_synkI0zL"
      },
      "source": [
        "### 2.3 POS-tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Например, NLTK"
      ],
      "metadata": {
        "id": "RDlA9cKaRG0g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-GoH0zA9wPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4628721-0fd6-4418-d398-fb333fcd4b55"
      },
      "source": [
        "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "tagged"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('At', 'IN'),\n",
              " ('eight', 'CD'),\n",
              " (\"o'clock\", 'NN'),\n",
              " ('on', 'IN'),\n",
              " ('Thursday', 'NNP'),\n",
              " ('morning', 'NN'),\n",
              " ('Arthur', 'NNP'),\n",
              " ('did', 'VBD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('feel', 'VB'),\n",
              " ('very', 'RB'),\n",
              " ('good', 'JJ'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7nhS1CpuTVL"
      },
      "source": [
        "Но только на английском... Какие еще постеггеры вы знаете?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download ru_core_news_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R9lKLAjZR8VE",
        "outputId": "61c31300-e062-482b-8e64-dd2adc7dd86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "doc = nlp(sent)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-1tpKQxo8Va",
        "outputId": "ad3e6d43-5d9e-41d7-c6f6-9ae125de6f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Для ADP\n",
            "безе NOUN\n",
            "надо ADV\n",
            "взбить VERB\n",
            "белки NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-TIn9iiB6Zc"
      },
      "source": [
        "### 2.4 Чанкинг и более сложная синтаксическая обработка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG4hijQyJlt1"
      },
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1bgUdg7vqbU4PgqiNoHbX81cz1fmIva5e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b66DYeQeLEMm"
      },
      "source": [
        "https://drive.google.com/file/d/1bgUdg7vqbU4PgqiNoHbX81cz1fmIva5e/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt1vMbahLa2R"
      },
      "source": [
        "Давайте при думаем правила простого чанкинга линейных именных групп в английском посредством nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjqbPsDcCctD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206de0ff-000f-40e6-d237-684a0920898f"
      },
      "source": [
        "sentence = [\n",
        "    (\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"),\n",
        "    (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")\n",
        "]\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(sentence)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT cat/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXllNzGmQBI"
      },
      "source": [
        "Можно ли для русского синтаксиса придумать подобные правила на основе частей речи? Возьмем предложение:\n",
        "\"Богатый мужчина дал золотых монет бедному\"\n",
        "\n",
        "1. Давайте попробуем разделить его на чанки.\n",
        "2. В каком формате это можно выводить?\n",
        "3. Напишите на основе наших размышлений код."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm82vvYYlJpZ"
      },
      "source": [
        "sent = 'Богатый мужчина дал золотых монет бедному'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJjPIdNBLxkn"
      },
      "source": [
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bz2_hI8L4BI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0b01fe-050d-4c89-c0c7-bed90ba6e75f"
      },
      "source": [
        "p = morph.parse('стали') [0]\n",
        "p.tag\n",
        "#p.tag.case\n",
        "#p.tag.gender\n",
        "#p.tag.number\n",
        "#p.tag.POS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpencorporaTag('VERB,perf,intr plur,past,indc')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKmwswXals8I"
      },
      "source": [
        "ent = ('NOUN', 'ADJF', 'PRTF')\n",
        "\n",
        "def chunking(sent):\n",
        "    chunked_sent = []\n",
        "    chunk = []\n",
        "\n",
        "    first_word = sent[0]\n",
        "    p = morph.parse(first_word)[0]\n",
        "    word_case = p.tag.case\n",
        "    number = p.tag.number\n",
        "    if p.tag.POS in ent:\n",
        "        chunk.append(first_word)\n",
        "    else:\n",
        "        chunked_sent.append(first_word)\n",
        "\n",
        "    for word in sent[1:]:\n",
        "        p = morph.parse(word)[0]\n",
        "        if chunk != []:\n",
        "            if p.tag.POS in ent:\n",
        "                if ((word_case ==  p.tag.case)  and (number == p.tag.number)):\n",
        "                    chunk.append(word)\n",
        "                else:\n",
        "                    chunked_sent.append(chunk)\n",
        "                    chunk = []\n",
        "                    chunk.append(word)\n",
        "                    word_case = p.tag.case\n",
        "                    number = p.tag.number\n",
        "            else:\n",
        "                chunked_sent.append(chunk)\n",
        "                chunk = []\n",
        "\n",
        "        if chunk == []:\n",
        "            if p.tag.POS in ent:\n",
        "                chunk.append(word)\n",
        "                word_case = p.tag.case\n",
        "                number = p.tag.number\n",
        "            else:\n",
        "                chunked_sent.append(word)\n",
        "\n",
        "    if chunk != []:\n",
        "        chunked_sent.append(chunk)\n",
        "    return chunked_sent\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0IxmZmp0TK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc19d96-afc0-42fd-fcbf-c05093f56a66"
      },
      "source": [
        "chunked_sent = chunking(sent.split())\n",
        "chunked_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Богатый', 'мужчина'], 'дал', ['золотых', 'монет'], ['бедному']]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EM7DkRBs1D-"
      },
      "source": [
        "Однако, что делать с различного роди причастными оборотами или предложениями с \"который\"? Тут уже не обойтись без деревьев. В python есть библиотеки типа UD_Pipe, которые рисуют готовое дерево - мы рассмотрим их подробнее на следующих занятиях по теме.\n",
        "\n",
        "> Под \"чанкингом\" также часто имеют в виду разделение текста (или аудио, например) на куски, чтобы было проще обучать нейронные сети. Например, у BERT'а ограничен размер входного сообщения до 512 токенов, поэтому для обработки очень больших текстов требуется разделить его на куски, т.е. чанки\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctxh5Gxf_Lsz"
      },
      "source": [
        "### 2.4 Словари и сеты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhUDb2Swu5Uk"
      },
      "source": [
        "1. В чём принципиальная разница между множеством (сетом) и массивом из уникальных элементов?\n",
        "2. Существуют ли сценарии при которых нужно использовать массив из уникальных элементов?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = clean_text.lower()\n",
        "\n",
        "no_accent = ['а', 'я', 'у', 'ю', 'о', 'е', 'э', 'и', 'ы', 'А', 'Я', 'У', 'Ю', 'О', 'Е', 'Э', 'И']\n",
        "accented = ['а́', 'я́', 'у́', 'ю́', 'о́', 'е́', 'э́', 'и́', 'ы́', 'А́', 'Я́', 'У́', 'Ю́', 'О́', 'Е́', 'Э́', 'И́']\n",
        "\n",
        "for acc, no_acc in zip(accented, no_accent):\n",
        "    clean_text = clean_text.replace(acc, no_acc)"
      ],
      "metadata": {
        "id": "vzjxsv2MeXH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0CHnRmkxDQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267280dd-5e98-4211-ead1-7c8ad825f084"
      },
      "source": [
        "tokens = nltk.word_tokenize(clean_text.lower())\n",
        "tokens = [token.strip(punctuation) for token in tokens if token.strip(punctuation)!='']\n",
        "tokens[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['лингвистика',\n",
              " 'от',\n",
              " 'лат',\n",
              " 'lingua',\n",
              " 'язык',\n",
              " 'языкознание',\n",
              " 'языковедение',\n",
              " 'наука',\n",
              " 'изучающая',\n",
              " 'язык']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzM4DKD3z0oF"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "WORDS = Counter()\n",
        "WORDS.update(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WORDS"
      ],
      "metadata": {
        "id": "Q1auQCQRuxpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knwOf4t20ZKL"
      },
      "source": [
        "## 3. Анализ и работа с данными\n",
        "\n",
        "Для работы с данными нет единого механизма - тут могут пригодиться и нейросети, и правила, стороннние библиотеки, и много чего еще - в рамках данного занятия посмотрим на вектора и расстояние Левенштейна."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d11loGqyTJv"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "from collections import Counter\n",
        "import textdistance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFZj1nFd1LEo"
      },
      "source": [
        "### Векторные методы\n",
        "\n",
        "1. Какие методы преобразования тексты в вектора (векторайзеры) вы знаете?\n",
        "2. Можете описать \"на пальцах\" как работает векторайзер?\n",
        "3. Как сравнить насколько вектора похожи?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(WORDS.keys())\n",
        "id2word = {i:word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "wzd_w_KdvJot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mfZTSLA1DaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8836a8be-62ea-4dac-d7e2-b34b4792e563"
      },
      "source": [
        "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
        "X = vec.fit_transform(vocab)\n",
        "\n",
        "def get_closest_match_vec(text, X, vec, TOPN=5):\n",
        "    v = vec.transform([text])\n",
        "    similarities = cosine_distances(v, X)\n",
        "    topn = similarities.argsort()[0][:TOPN]\n",
        "\n",
        "    return [id2word[top] for top in topn]\n",
        "\n",
        "get_closest_match_vec('асновного', X, vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['основного', 'основана', 'иностранного', 'оно', 'но']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdKcrmtm1Z9N"
      },
      "source": [
        "### Расстояние Левенштейна\n",
        "\n",
        "1. Что такое расстояние Левенштейна?\n",
        "2. Какие у него плюсы и минусы?\n",
        "\n",
        "![Изображение с ресурса \"Системный Блокъ\"](https://sysblok.ru/wp-content/uploads/2020/12/image1-2.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoY93kJ1DkM"
      },
      "source": [
        "def get_closest_match_with_metric(text, X, vec, metric=textdistance.levenshtein):\n",
        "    similarities = Counter()\n",
        "    lookup = get_closest_match_vec(text, X, vec, TOPN=3)\n",
        "    for word in lookup:\n",
        "        similarities[word] = metric.normalized_similarity(text, word)\n",
        "\n",
        "    return similarities.most_common(1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJfFle3-yIeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04c4d30-d2db-4d13-bd80-e37b91274930"
      },
      "source": [
        "get_closest_match_with_metric('ленгвистика', X, vec, textdistance.hamming)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('лингвистика', 0.9090909090909091)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olVt4Ka2zKx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc27ef6-3c95-4391-eaf6-049e1b1e527f"
      },
      "source": [
        "get_closest_match_with_metric('импирическая', X, vec, textdistance.hamming)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('эмпирическая', 0.9166666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXKVbW4azayt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea70d4f-63f8-410a-8100-de43ab86d3b4"
      },
      "source": [
        "get_closest_match_with_metric('мадэлей', X, vec, textdistance.hamming)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('моделей', 0.7142857142857143)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2x3LOuf4mFt"
      },
      "source": [
        "При выборе модели решения задачи мы всегда осуществляем некий трейд-офф между эффективность и время- энерго- затратностью. Алгоритмы, которые работают \"качественнее\"/ \"эффективнее\" часто требуют больших мощностей или времени.\n",
        "\n",
        "А как кстати определить качество? И как его улучшить?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZlNpRwH5JCH"
      },
      "source": [
        "### 4. Качество\n",
        "\n",
        "Для оценки качества подхода требуется большое количество данных и правильно подобранные метрики. Базовые метрики: __accuracy, precision, recall, f1__.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1ibvBKRAy_mn-ln4iZfzJNFTm349HCTmT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $F_{1}=\\frac{2\\times\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
        "\n",
        "## $Accuracy=\\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$"
      ],
      "metadata": {
        "id": "i6NRuEqEprYt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z68G1I0_SBdo"
      },
      "source": [
        "Можно ли найти уже готовый текст, на котором мы можем проверить наш спеллчекер?\n",
        "\n",
        "На большой коллекции текстов с сайтов интернет-газет и размеченных тектов диалога получились следующие метрики:\n",
        "- precision ~ 40%,\n",
        "- recall ~ 60%\n",
        "\n",
        "Что значат эти показатели с точки зрения работы модели и имеющихся данных? Как улучшить эти показатели?\n",
        "\n",
        "Давайте посмотрим на ошибки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRAlYPj8TMNw"
      },
      "source": [
        "Так, среди ошибок\n",
        "\n",
        "['симпатичнейшое', 'пояним', 'полчатся', 'нащщот', 'основая', 'вобщем', 'как', 'вы', 'знаете', 'из', 'моего', 'не', 'давнего', 'ящека', 'хороше', 'патаму', 'шта', 'поффтыкав', 'соре', 'чтото', 'мошный', 'хороше', 'седня', 'вешать', 'эт', 'канешна', 'начальнег', 'павзрослому', 'подсаживаеться', 'какието', 'малолетки', 'монголоидом', 'баръер', 'коментариев', 'отвественный', 'расчитаны', 'обълись', 'распрашивая', 'зубодробительня', 'каааак', 'расщифровать', 'самойто', 'тул', 'ваще', 'тока', 'навернео', 'ооочень', 'както', 'мущщину', 'никада']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get_closest_match_with_metric('выигрышь', X, vec, textdistance.hamming)"
      ],
      "metadata": {
        "id": "o8sxhkk02o8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IorxQvb9YrHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}