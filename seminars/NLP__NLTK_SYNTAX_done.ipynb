{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "NLP _NLTK_SYNTAX_done.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTY4l42d8GoG",
        "outputId": "9d1c26eb-9190-4822-bbd4-216129397d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1SVhzHsD5r1"
      },
      "source": [
        "import itertools"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFurUsKyA92"
      },
      "source": [
        "nltk.download('all-corpora')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KteuSNvolccE"
      },
      "source": [
        "## Инструменты для работы в циклах (итераторы)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xqicII6ELdC"
      },
      "source": [
        "### zip и zip_longest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzb6rUXglhxu",
        "outputId": "1b14b78d-0453-439e-dd3c-0ec4f36074fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for a, b in zip ([1,2,3], [1,2,3]):\n",
        "  print (a+b)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNQAEcuvE1CE"
      },
      "source": [
        "Также с индексами (переюор массива циклом немножко быстрее, чем обращение по адрессу?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePrJ-i6_DSc-"
      },
      "source": [
        "про то, что могут быть неравными"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQjwUUXD2wO",
        "outputId": "409c1e41-e830-4800-809e-ac077809874e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for t in itertools.zip_longest('ABCD', 'xy', fillvalue='-'):\n",
        "  print(t)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('A', 'x')\n",
            "('B', 'y')\n",
            "('C', '-')\n",
            "('D', '-')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDCrfBCaDDg_"
      },
      "source": [
        "zipped = zip([1,2,3], [1,2,3])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z87Oeos5DI1F",
        "outputId": "92994f58-e28c-4195-bd46-57c780b6d186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(zipped)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (2, 2), (3, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of2IfoQ7EUCi"
      },
      "source": [
        "### Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUQPepn9EjiA",
        "outputId": "53490003-fd2d-403b-d0ba-ff6314dba5ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "for t in itertools.chain('ABCD', 'xy'):\n",
        "  print(t)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "x\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jbIvaxHEvzT",
        "outputId": "97c078f6-24ec-44c6-b3ac-92fb95f014a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "for t in itertools.chain.from_iterable(['ABCD', 'xy', 'we']):\n",
        "  print(t)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "x\n",
            "y\n",
            "w\n",
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-_eASQJFxZT"
      },
      "source": [
        "### Всякие фильтры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0T-D7r2F0ps",
        "outputId": "54295eac-45c0-4582-b2a4-2a16e075619f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for t in itertools.compress('ABCDEF', [1,0,1,0,1,1]):\n",
        "  print(t)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "C\n",
            "E\n",
            "F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnJVJRU-GDGp",
        "outputId": "1ae73a5e-7d12-449f-e01b-d84b5d269f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for t in itertools.dropwhile(lambda x: x<5, [1,4,6,4,1]):\n",
        "  print(t)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "4\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJaax5ncGPx4"
      },
      "source": [
        "Вы же знаете, что такое лямбда ?\n",
        "\n",
        "def lambda ( АРГУМЕНТЫ):\n",
        "\n",
        "return TRUE ИЛИ FALSE OТ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYNmqpvCGZJB",
        "outputId": "20bef355-8d63-4993-dd8c-a2f889ae1626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "for t in itertools.takewhile(lambda x: x<5, [1,4,6,4,1]):\n",
        "  print(t)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuKp7GJ5GhMQ",
        "outputId": "3c7cf391-7d82-4d84-a940-3a088c02ed12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for t in itertools.filterfalse(lambda x: x%2, range(10)):\n",
        "  print(t)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZB8GCVIqfT"
      },
      "source": [
        "Документация https://docs.python.org/3/library/itertools.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMta6Bhk8GoR"
      },
      "source": [
        "## Синтаксис в NLTK\n",
        " #### Грамматика составляющих (помните такую?)\n",
        "\n",
        "\n",
        "NLTK умеет не только в морфу, но и в синтаксис. Помните, на самом первом занятии, когда мы занимались чанкингом (кстати, это що?), мы быстро начертили какое-то стрёмное дерево и побежали дальше?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fka8-pHOmNuq",
        "outputId": "8bc985cc-a41c-4d54-a299-552c7f0596c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
        "\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(sentence)\n",
        "print(result)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT cat/NN))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkQ_AdcAmxVU"
      },
      "source": [
        "Так вот сегодня будем разбираться со всем этим подробнее"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mXhs4s68GoT"
      },
      "source": [
        "rules = \"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det ADJ N | Det N | NN\n",
        "    VP -> V NP \n",
        "    Det -> 'a'\n",
        "    ADJ -> 'tasty' | ADV ADJ\n",
        "    ADV -> 'very'\n",
        "    N -> 'fish' | 'fork' | 'dog' | 'boy'\n",
        "    NN -> 'Mary' | 'John'\n",
        "    V -> 'eats'\n",
        "\"\"\".split('\\n')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6O-gXT6sLZJ"
      },
      "source": [
        "# сделали парсер\n",
        "grammar = nltk.CFG.fromstring('\\n'.join(rules))\n",
        "cp = nltk.EarleyChartParser(grammar)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J1fBtMf8GpO",
        "outputId": "ad5cc548-46bd-4c9d-af45-46ef88c50db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def print_parses(parser, sentence):\n",
        "    for tree in parser.parse(sentence.split()):\n",
        "        print(tree)\n",
        "        \n",
        "print_parses(cp, \"Mary eats a fish\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP (NN Mary)) (VP (V eats) (NP (Det a) (N fish))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aPmo-F1srKv"
      },
      "source": [
        "Давайте посмотрим на иконичное предложение с 2 прочтениями I shot elephant in my pajamas. NLTK умеет выдавать сразу несколько деревьев, если вы ему дадите предложение с возможной неоднозначностью -- главное настроить так грамматику, чтобы он ловил эту неоднозначность и не ломался.\n",
        "\n",
        "Вывод ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoV0m5KSekqY"
      },
      "source": [
        "\n",
        "\n",
        ">>> el_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "\n",
        "# Your Code here\n",
        "... S -> NP VP\n",
        "... PP -> P NP\n",
        "... NP -> Det N | NP PP | 'I'\n",
        "... VP -> V NP | VP PP\n",
        "... Det -> 'an' | 'my'\n",
        "... N -> 'elephant' | 'pajamas'\n",
        "... V -> 'shot'\n",
        "... P -> 'in'\n",
        "... \"\"\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJ7JL9stvjL"
      },
      "source": [
        "Заметьте, что парсер поменялся -- там и в NLTK штук 10, но по факту отличаеются они мало."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbqxpIrwev-r",
        "outputId": "852db63e-b184-46b2-f7ef-574c8c29b9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        ">>> sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        ">>> parser = nltk.ChartParser(el_grammar) #\n",
        ">>> for tree in parser.parse(sent):\n",
        "...     print(tree)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V shot) (NP (Det an) (N elephant)))\n",
            "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V shot)\n",
            "    (NP\n",
            "      (NP (Det an) (N elephant))\n",
            "      (PP (P in) (NP (Det my) (N pajamas))))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_jQ32fu8GpT"
      },
      "source": [
        "Вернемся к описанию EarleyChartParser и найдём, как включить отладочный вывод -- чтобы посмотреть на шаги разбора."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC2mA-Gg8GpU",
        "outputId": "9d6c5d2b-e313-4af6-ad6e-b58790c7b65d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "cp_verbose = nltk.EarleyChartParser(grammar, trace=1)\n",
        "print_parses(cp_verbose, \"Mary eats a fish\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|.   Mary  .   eats  .    a    .   fish  .|\n",
            "|[---------]         .         .         .| [0:1] 'Mary'\n",
            "|.         [---------]         .         .| [1:2] 'eats'\n",
            "|.         .         [---------]         .| [2:3] 'a'\n",
            "|.         .         .         [---------]| [3:4] 'fish'\n",
            "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
            "|>         .         .         .         .| [0:0] NP -> * Det ADJ N\n",
            "|>         .         .         .         .| [0:0] NP -> * Det N\n",
            "|>         .         .         .         .| [0:0] NP -> * NN\n",
            "|>         .         .         .         .| [0:0] NN -> * 'Mary'\n",
            "|[---------]         .         .         .| [0:1] NN -> 'Mary' *\n",
            "|[---------]         .         .         .| [0:1] NP -> NN *\n",
            "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
            "|.         >         .         .         .| [1:1] VP -> * V NP\n",
            "|.         >         .         .         .| [1:1] V  -> * 'eats'\n",
            "|.         [---------]         .         .| [1:2] V  -> 'eats' *\n",
            "|.         [--------->         .         .| [1:2] VP -> V * NP\n",
            "|.         .         >         .         .| [2:2] NP -> * Det ADJ N\n",
            "|.         .         >         .         .| [2:2] NP -> * Det N\n",
            "|.         .         >         .         .| [2:2] NP -> * NN\n",
            "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
            "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
            "|.         .         [--------->         .| [2:3] NP -> Det * ADJ N\n",
            "|.         .         [--------->         .| [2:3] NP -> Det * N\n",
            "|.         .         .         >         .| [3:3] N  -> * 'fish'\n",
            "|.         .         .         >         .| [3:3] ADJ -> * ADV ADJ\n",
            "|.         .         .         [---------]| [3:4] N  -> 'fish' *\n",
            "|.         .         [-------------------]| [2:4] NP -> Det N *\n",
            "|.         [-----------------------------]| [1:4] VP -> V NP *\n",
            "|[=======================================]| [0:4] S  -> NP VP *\n",
            "(S (NP (NN Mary)) (VP (V eats) (NP (Det a) (N fish))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2im1Ey58GpY",
        "outputId": "1ca9e45c-e7b2-4ad4-b2f7-0e1d9ff43a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Можно больше отладочного вывода!\n",
        "very_verbose_cp = nltk.EarleyChartParser(grammar, trace=2)\n",
        "print_parses(very_verbose_cp, \"Mary eats a very tasty fish\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|. Mary . eats .  a   . very .tasty . fish .|\n",
            "Leaf Init Rule:\n",
            "|[------]      .      .      .      .      .| [0:1] 'Mary'\n",
            "|.      [------]      .      .      .      .| [1:2] 'eats'\n",
            "|.      .      [------]      .      .      .| [2:3] 'a'\n",
            "|.      .      .      [------]      .      .| [3:4] 'very'\n",
            "|.      .      .      .      [------]      .| [4:5] 'tasty'\n",
            "|.      .      .      .      .      [------]| [5:6] 'fish'\n",
            "Top Down Init Rule:\n",
            "|>      .      .      .      .      .      .| [0:0] S  -> * NP VP\n",
            "\n",
            "* Processing queue: 0 \n",
            "\n",
            "Predictor Rule:\n",
            "|>      .      .      .      .      .      .| [0:0] NP -> * Det ADJ N\n",
            "|>      .      .      .      .      .      .| [0:0] NP -> * Det N\n",
            "|>      .      .      .      .      .      .| [0:0] NP -> * NN\n",
            "Predictor Rule:\n",
            "|>      .      .      .      .      .      .| [0:0] NN -> * 'Mary'\n",
            "\n",
            "* Processing queue: 1 \n",
            "\n",
            "Scanner Rule:\n",
            "|[------]      .      .      .      .      .| [0:1] NN -> 'Mary' *\n",
            "Completer Rule:\n",
            "|[------]      .      .      .      .      .| [0:1] NP -> NN *\n",
            "Completer Rule:\n",
            "|[------>      .      .      .      .      .| [0:1] S  -> NP * VP\n",
            "Predictor Rule:\n",
            "|.      >      .      .      .      .      .| [1:1] VP -> * V NP\n",
            "Predictor Rule:\n",
            "|.      >      .      .      .      .      .| [1:1] V  -> * 'eats'\n",
            "\n",
            "* Processing queue: 2 \n",
            "\n",
            "Scanner Rule:\n",
            "|.      [------]      .      .      .      .| [1:2] V  -> 'eats' *\n",
            "Completer Rule:\n",
            "|.      [------>      .      .      .      .| [1:2] VP -> V * NP\n",
            "Predictor Rule:\n",
            "|.      .      >      .      .      .      .| [2:2] NP -> * Det ADJ N\n",
            "|.      .      >      .      .      .      .| [2:2] NP -> * Det N\n",
            "|.      .      >      .      .      .      .| [2:2] NP -> * NN\n",
            "Predictor Rule:\n",
            "|.      .      >      .      .      .      .| [2:2] Det -> * 'a'\n",
            "\n",
            "* Processing queue: 3 \n",
            "\n",
            "Scanner Rule:\n",
            "|.      .      [------]      .      .      .| [2:3] Det -> 'a' *\n",
            "Completer Rule:\n",
            "|.      .      [------>      .      .      .| [2:3] NP -> Det * ADJ N\n",
            "|.      .      [------>      .      .      .| [2:3] NP -> Det * N\n",
            "Predictor Rule:\n",
            "|.      .      .      >      .      .      .| [3:3] ADJ -> * ADV ADJ\n",
            "Predictor Rule:\n",
            "|.      .      .      >      .      .      .| [3:3] ADV -> * 'very'\n",
            "\n",
            "* Processing queue: 4 \n",
            "\n",
            "Scanner Rule:\n",
            "|.      .      .      [------]      .      .| [3:4] ADV -> 'very' *\n",
            "Completer Rule:\n",
            "|.      .      .      [------>      .      .| [3:4] ADJ -> ADV * ADJ\n",
            "Predictor Rule:\n",
            "|.      .      .      .      >      .      .| [4:4] ADJ -> * 'tasty'\n",
            "|.      .      .      .      >      .      .| [4:4] ADJ -> * ADV ADJ\n",
            "\n",
            "* Processing queue: 5 \n",
            "\n",
            "Scanner Rule:\n",
            "|.      .      .      .      [------]      .| [4:5] ADJ -> 'tasty' *\n",
            "Completer Rule:\n",
            "|.      .      .      [-------------]      .| [3:5] ADJ -> ADV ADJ *\n",
            "Completer Rule:\n",
            "|.      .      [-------------------->      .| [2:5] NP -> Det ADJ * N\n",
            "Predictor Rule:\n",
            "|.      .      .      .      .      >      .| [5:5] N  -> * 'fish'\n",
            "\n",
            "* Processing queue: 6 \n",
            "\n",
            "Scanner Rule:\n",
            "|.      .      .      .      .      [------]| [5:6] N  -> 'fish' *\n",
            "Completer Rule:\n",
            "|.      .      [---------------------------]| [2:6] NP -> Det ADJ N *\n",
            "Completer Rule:\n",
            "|.      [----------------------------------]| [1:6] VP -> V NP *\n",
            "Completer Rule:\n",
            "|[=========================================]| [0:6] S  -> NP VP *\n",
            "(S\n",
            "  (NP (NN Mary))\n",
            "  (VP (V eats) (NP (Det a) (ADJ (ADV very) (ADJ tasty)) (N fish))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dooZKWwIvQdz"
      },
      "source": [
        "If you are interested in experimenting with writing CFGs, you will find it helpful to create and edit your grammar in a text file, say mygrammar.cfg. You can then load it into NLTK and parse with it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe5lDZkjvsaH",
        "outputId": "77d0deeb-9210-44c9-e9ea-5c98411e59ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        ">>> grammar1 = nltk.data.load('file:mygrammar.cfg') #у нас его нет\n",
        ">>> sent = \"Mary saw Bob\".split()\n",
        ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        ">>> for tree in rd_parser.parse(sent):\n",
        "...      print(tree)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d935665295f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrammar1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file:mygrammar.cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Mary saw Bob\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrd_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveDescentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcontent\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('content')\n  \u001b[0m\n  Searched in:\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LoDJDvfvoF2"
      },
      "source": [
        "When you write CFGs for parsing in NLTK, you cannot combine grammatical categories with lexical items on the righthand side of the same production. Thus, a production such as PP -> 'of' NP is disallowed. In addition, you are not permitted to place multi-word lexical items on the righthand side of a production. So rather than writing NP -> 'New\n",
        "York', you have to resort to something like NP -> 'New_York' instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7Fa-wtv8Gp9"
      },
      "source": [
        "## Грамматики зависимостей ( а это что?)\n",
        "Можно использовать то же предложение, поскольку неоднозначность проявляется и в представлении зависимостей.\n",
        "В NLTK можно примерно одинаково работать с CFG и зависимостями (`DependencyGrammar`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPdyce_X8Gp-"
      },
      "source": [
        "dep_rules = \"\"\"\n",
        "... 'shot' -> 'I' | 'elephant' | 'in' | 'morning'\n",
        "... 'morning' -> 'One'\n",
        "... 'elephant' -> 'an' | 'in'\n",
        "... 'in' -> 'pajamas'\n",
        "... 'pajamas' -> 'my'\n",
        "... \"\"\""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzkYE_ym8GqB"
      },
      "source": [
        "dep_grammar = nltk.DependencyGrammar.fromstring(dep_rules)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEvVDkHa8GqH",
        "outputId": "00e17d72-4383-4a98-d0c0-2f7989d8c3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pdp = nltk.ProjectiveDependencyParser(dep_grammar)\n",
        "sent = 'One morning I shot an elephant in my pajamas'\n",
        "print_parses(pdp, sent)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(shot (morning One) I (elephant an (in (pajamas my))))\n",
            "(shot (morning One) I (elephant an) (in (pajamas my)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffel_vRQ8Gqe"
      },
      "source": [
        "## Встроенные грамматики NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks3XmBzbxIKF",
        "outputId": "3099b73e-3402-4894-ce95-9f573812a564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        ">>> from nltk.corpus import treebank\n",
        ">>> t = treebank.parsed_sents('wsj_0001.mrg')\n",
        ">>> print(t)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Tree('S', [Tree('NP-SBJ', [Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]), Tree(',', [',']), Tree('ADJP', [Tree('NP', [Tree('CD', ['61']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])]), Tree(',', [','])]), Tree('VP', [Tree('MD', ['will']), Tree('VP', [Tree('VB', ['join']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['board'])]), Tree('PP-CLR', [Tree('IN', ['as']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['nonexecutive']), Tree('NN', ['director'])])]), Tree('NP-TMP', [Tree('NNP', ['Nov.']), Tree('CD', ['29'])])])]), Tree('.', ['.'])]), Tree('S', [Tree('NP-SBJ', [Tree('NNP', ['Mr.']), Tree('NNP', ['Vinken'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP-PRD', [Tree('NP', [Tree('NN', ['chairman'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('NP', [Tree('NNP', ['Elsevier']), Tree('NNP', ['N.V.'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['the']), Tree('NNP', ['Dutch']), Tree('VBG', ['publishing']), Tree('NN', ['group'])])])])])]), Tree('.', ['.'])])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG66IoYvzO1o"
      },
      "source": [
        "Посмотрите на эту программу и скажите, что она делает?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaF3mNpWyGca"
      },
      "source": [
        "def filter(tree):\n",
        "    child_nodes = [child.label() for child in tree\n",
        "                   if isinstance(child, nltk.Tree)]\n",
        "    return  (tree.label() == 'VP') and ('S' in child_nodes)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApC0-6cpyNb_",
        "outputId": "da813765-b0f2-49b2-d30f-e2c2387bb4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "for tr in t:\n",
        "  print(tr)\n",
        "  print(filter(tr))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP-SBJ\n",
            "    (NP (NNP Pierre) (NNP Vinken))\n",
            "    (, ,)\n",
            "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
            "    (, ,))\n",
            "  (VP\n",
            "    (MD will)\n",
            "    (VP\n",
            "      (VB join)\n",
            "      (NP (DT the) (NN board))\n",
            "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
            "      (NP-TMP (NNP Nov.) (CD 29))))\n",
            "  (. .))\n",
            "False\n",
            "(S\n",
            "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
            "  (VP\n",
            "    (VBZ is)\n",
            "    (NP-PRD\n",
            "      (NP (NN chairman))\n",
            "      (PP\n",
            "        (IN of)\n",
            "        (NP\n",
            "          (NP (NNP Elsevier) (NNP N.V.))\n",
            "          (, ,)\n",
            "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
            "  (. .))\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_KLeYOpzh8s"
      },
      "source": [
        "Окей, но что нам делать с неоднозначностью?\n",
        "\n",
        "Когда размер грамматики увеличивается, то пропорционально увеливается и количество разборов. Не редко NLTK дают ложные разборы (потому что относительно точную грамматику составить, конечно, можно, но сложно)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZiOzLK60RXq"
      },
      "source": [
        "Давайте возьмём иконичный (дебильный) пример из NLTK.\n",
        "\n",
        " Преложение fish fish fish fish fis, которое, кстати говоря, является абсолютно грамматическим английским предложением неоднозначно в нашей грамматике. Как автоматически понять, какой разбор лучше?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tkk7V5Jz-hq",
        "outputId": "2618a047-d226-4a20-896e-e68e71162da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        ">>> grammar = nltk.CFG.fromstring(\"\"\"\n",
        "... S -> NP V NP\n",
        "... NP -> NP Sbar\n",
        "... Sbar -> NP V\n",
        "... NP -> 'fish'\n",
        "... V -> 'fish'\n",
        "... \"\"\")\n",
        "\n",
        "# 'fish that other fish fish are in the habit of fishing fish themselves'.\n",
        ">>> tokens = [\"fish\"] * 5\n",
        ">>> cp = nltk.ChartParser(grammar)\n",
        ">>> for tree in cp.parse(tokens):\n",
        "...     print(tree)\n",
        "# ну и какое верное, кстати говоря?"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\n",
            "(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cawbvIN16Fjq"
      },
      "source": [
        "какие есть решения?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXoE2Er-6IOu"
      },
      "source": [
        "1. Особые классы глаголов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFf8gxE8zbPw"
      },
      "source": [
        "rule_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "VPpred -> Vpred Adj\n",
        "VPn -> Vn NP\t\n",
        "VPs -> Vs S\t\n",
        "VPphrase -> Vphrase NP PP\t\n",
        "Vpred -> 'was'\n",
        "Vn -> 'saw'\n",
        "Vs -> 'thought'\n",
        "Vphrase -> 'put'\n",
        "\n",
        " \"\"\")\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcFmSrA65FG"
      },
      "source": [
        "2. Weighted grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpSc-tni7Pcu"
      },
      "source": [
        "У нас есть большой корпус деревьев в NLTK. Можно посчитать вероятность каждой конструкции. Чуть выше мы видели код, который найдет все Vp -> V S. Можно такие вычисления проделать для всего и добавить это в нашу грамматику в таком формате."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olnVkagL61aW"
      },
      "source": [
        "\t\n",
        "grammar = nltk.PCFG.fromstring(\"\"\"\n",
        "    S    -> NP VP              [1.0]\n",
        "    VP   -> TV NP              [0.4]\n",
        "    VP   -> IV                 [0.3]\n",
        "    VP   -> DatV NP NP         [0.3]\n",
        "    TV   -> 'saw'              [1.0]\n",
        "    IV   -> 'ate'              [1.0]\n",
        "    DatV -> 'gave'             [1.0]\n",
        "    NP   -> 'telescopes'       [0.8]\n",
        "    NP   -> 'Jack'             [0.2]\n",
        "    \"\"\")"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9KYnK9f76Dq",
        "outputId": "70f239c2-1b5d-45da-c560-45555b2389a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ">>> viterbi_parser = nltk.ViterbiParser(grammar) #снова новый парсер\n",
        ">>> for tree in viterbi_parser.parse(['Jack', 'saw', 'telescopes']):\n",
        "...     print(tree)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHLlLT98OGg"
      },
      "source": [
        "Задание 1.\n",
        "\n",
        "Любым доступным способом сделайте дерево для  Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo. Посмотрите на дерево по ссылке и напишите грамматику, чтобы получилось также и был только один вариант  http://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrrKWH5h9AIu"
      },
      "source": [
        "# Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvCxYgJ59DBm"
      },
      "source": [
        "3. Морфология"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wxvt2m1BMOw",
        "outputId": "defbcd99-f989-4248-f20f-2548765e6f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\n",
        "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\n",
        "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\n",
        "\n",
        "V[SUBCAT=intrans, TENSE=pres, NUM=sg] -> 'disappears' | 'walks'\n",
        "V[SUBCAT=trans, TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
        "V[SUBCAT=clause, TENSE=pres, NUM=sg] -> 'says' | 'claims'\n",
        "\n",
        "V[SUBCAT=intrans, TENSE=pres, NUM=pl] -> 'disappear' | 'walk'\n",
        "V[SUBCAT=trans, TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
        "V[SUBCAT=clause, TENSE=pres, NUM=pl] -> 'say' | 'claim'\n",
        "\n",
        "V[SUBCAT=intrans, TENSE=past, NUM=?n] -> 'disappeared' | 'walked'\n",
        "V[SUBCAT=trans, TENSE=past, NUM=?n] -> 'saw' | 'liked'\n",
        "V[SUBCAT=clause, TENSE=past, NUM=?n] -> 'said' | 'claimed'"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-c6838c330fbd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGFr3Z1CBReQ"
      },
      "source": [
        "4. Всё вместе"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryf4fMIBmIx"
      },
      "source": [
        "## Разумеется, всё у нас уже готово в NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57bUg9oYBxFU",
        "outputId": "46e15b33-3267-48d1-cb45-52b368aa79cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "nltk.download ('book_grammars')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package book_grammars to /root/nltk_data...\n",
            "[nltk_data]   Unzipping grammars/book_grammars.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yheQLOEqBs1-",
        "outputId": "baef0d61-db24-4acf-a7b7-04cff2a2b1d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% start S\n",
            "# ###################\n",
            "# Grammar Productions\n",
            "# ###################\n",
            "S[-INV] -> NP VP\n",
            "S[-INV]/?x -> NP VP/?x\n",
            "S[-INV] -> NP S/NP\n",
            "S[-INV] -> Adv[+NEG] S[+INV]\n",
            "S[+INV] -> V[+AUX] NP VP\n",
            "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
            "SBar -> Comp S[-INV]\n",
            "SBar/?x -> Comp S[-INV]/?x\n",
            "VP -> V[SUBCAT=intrans, -AUX]\n",
            "VP -> V[SUBCAT=trans, -AUX] NP\n",
            "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
            "VP -> V[SUBCAT=clause, -AUX] SBar\n",
            "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
            "VP -> V[+AUX] VP\n",
            "VP/?x -> V[+AUX] VP/?x\n",
            "# ###################\n",
            "# Lexical Productions\n",
            "# ###################\n",
            "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
            "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
            "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
            "V[+AUX] -> 'do' | 'can'\n",
            "NP[-WH] -> 'you' | 'cats'\n",
            "NP[+WH] -> 'who'\n",
            "Adv[+NEG] -> 'rarely' | 'never'\n",
            "NP/NP ->\n",
            "Comp -> 'that'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc5GR-ZoB9BO",
        "outputId": "0f5d847b-a8ee-450d-c35e-74404b654c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        ">>> tokens = 'who do you claim that you like'.split()\n",
        ">>> from nltk import load_parser\n",
        ">>> cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
        ">>> for tree in cp.parse(tokens):\n",
        "...     print(tree)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S[-INV]\n",
            "  (NP[+WH] who)\n",
            "  (S[+INV]/NP[]\n",
            "    (V[+AUX] do)\n",
            "    (NP[-WH] you)\n",
            "    (VP[]/NP[]\n",
            "      (V[-AUX, SUBCAT='clause'] claim)\n",
            "      (SBar[]/NP[]\n",
            "        (Comp[] that)\n",
            "        (S[-INV]/NP[]\n",
            "          (NP[-WH] you)\n",
            "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha0J2vos8GrQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}