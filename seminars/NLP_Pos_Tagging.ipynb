{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/site-packages (0.8)\r\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.7/site-packages (from pymorphy2) (2.4.393442.3710985)\r\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/site-packages (from pymorphy2) (0.6.2)\r\n",
      "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.7/site-packages (from pymorphy2) (0.7.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Мама мыла раму.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='мама мыла раму.', tag=OpencorporaTag('UNKN'), normal_form='мама мыла раму.', score=1.0, methods_stack=((<UnknAnalyzer>, 'Мама мыла раму.'),))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='мама', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='мама', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'мама', 1907, 0),))]\n",
      "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 1),)), Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 1813, 8),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 6),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 9),))]\n",
      "[Parse(word='раму.', tag=OpencorporaTag('UNKN'), normal_form='раму.', score=1.0, methods_stack=((<UnknAnalyzer>, 'раму.'),))]\n"
     ]
    }
   ],
   "source": [
    "for token in text.split():\n",
    "    print(m.parse(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='мама', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='мама', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'мама', 1907, 0),))]\n",
      "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 1),)), Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 1813, 8),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 6),)), Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 9),))]\n",
      "[Parse(word='раму', tag=OpencorporaTag('NOUN,inan,masc,Geox sing,datv'), normal_form='рам', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 32, 2),)), Parse(word='раму', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='рама', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 55, 3),))]\n",
      "[Parse(word='.', tag=OpencorporaTag('PNCT'), normal_form='.', score=1.0, methods_stack=((<PunctuationAnalyzer>, '.'),))]\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "for token in simple_word_tokenize(text):\n",
    "    print(m.parse(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Форматы корпусов\n",
    "\n",
    "Для простых задач часто подходит формат корпусов NLTK ([tagged_words](https://www.nltk.org/book/ch05.html)/tagged_sents).\n",
    "Корпуса с разметкой зависимостей и подробной морфологической разметкой часто хранятся в формате [CoNLL](https://universaldependencies.org/format.html).\n",
    "Его удобно читать библиотекой `conllu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in /usr/local/lib/python3.7/site-packages (2.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте скачаем часть корпуса Universal Dependencies для русского - СинТагРус\n",
    "import requests\n",
    "r = requests.get(\"https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-train.conllu\")\n",
    "train_corpus = r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = 2003Anketa.xml_1\n",
      "# text = Анкета.\n",
      "1\tАнкета\tанкета\tNOUN\t_\tAnimacy=Inan|Case=Nom|Gender=Fem|Number=Sing\t0\troot\t0:root\tSpaceAfter=No\n",
      "2\t.\t.\tPUNCT\t_\t_\t1\tpunct\t1:punct\t_\n",
      "\n",
      "# sent_id = 2003Anketa.xml_2\n",
      "# text = Начальник областного управления связи Семен Еремеевич был человек простой, приходил на работу всегда вовремя, здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \"Муха\".\n",
      "1\tНачальник\tначальник\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t8\t\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus.decode('utf-8')[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = conllu.parse(train_corpus.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48814"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('id', 1), ('form', 'Начальник'), ('lemma', 'начальник'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 8), ('deprel', 'nsubj'), ('deps', [('nsubj', 8)]), ('misc', None)])\n",
      "OrderedDict([('id', 2), ('form', 'областного'), ('lemma', 'областной'), ('upostag', 'ADJ'), ('xpostag', None), ('feats', OrderedDict([('Case', 'Gen'), ('Degree', 'Pos'), ('Gender', 'Neut'), ('Number', 'Sing')])), ('head', 3), ('deprel', 'amod'), ('deps', [('amod', 3)]), ('misc', None)])\n",
      "OrderedDict([('id', 3), ('form', 'управления'), ('lemma', 'управление'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Inan'), ('Case', 'Gen'), ('Gender', 'Neut'), ('Number', 'Sing')])), ('head', 1), ('deprel', 'nmod'), ('deps', [('nmod', 1)]), ('misc', None)])\n",
      "OrderedDict([('id', 4), ('form', 'связи'), ('lemma', 'связь'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Inan'), ('Case', 'Gen'), ('Gender', 'Fem'), ('Number', 'Sing')])), ('head', 3), ('deprel', 'nmod'), ('deps', [('nmod', 3)]), ('misc', None)])\n",
      "OrderedDict([('id', 5), ('form', 'Семен'), ('lemma', 'Семен'), ('upostag', 'PROPN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 1), ('deprel', 'appos'), ('deps', [('appos', 1)]), ('misc', None)])\n",
      "OrderedDict([('id', 6), ('form', 'Еремеевич'), ('lemma', 'Еремеевич'), ('upostag', 'PROPN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 5), ('deprel', 'flat:name'), ('deps', [('flat:name', 5)]), ('misc', None)])\n",
      "OrderedDict([('id', 7), ('form', 'был'), ('lemma', 'быть'), ('upostag', 'AUX'), ('xpostag', None), ('feats', OrderedDict([('Aspect', 'Imp'), ('Gender', 'Masc'), ('Mood', 'Ind'), ('Number', 'Sing'), ('Tense', 'Past'), ('VerbForm', 'Fin'), ('Voice', 'Act')])), ('head', 8), ('deprel', 'cop'), ('deps', [('cop', 8)]), ('misc', None)])\n",
      "OrderedDict([('id', 8), ('form', 'человек'), ('lemma', 'человек'), ('upostag', 'NOUN'), ('xpostag', None), ('feats', OrderedDict([('Animacy', 'Anim'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 0), ('deprel', 'root'), ('deps', '0:root'), ('misc', None)])\n",
      "OrderedDict([('id', 9), ('form', 'простой'), ('lemma', 'простой'), ('upostag', 'ADJ'), ('xpostag', None), ('feats', OrderedDict([('Case', 'Nom'), ('Degree', 'Pos'), ('Gender', 'Masc'), ('Number', 'Sing')])), ('head', 8), ('deprel', 'amod'), ('deps', [('amod', 8)]), ('misc', OrderedDict([('SpaceAfter', 'No')]))])\n",
      "OrderedDict([('id', 10), ('form', ','), ('lemma', ','), ('upostag', 'PUNCT'), ('xpostag', None), ('feats', None), ('head', 11), ('deprel', 'punct'), ('deps', [('punct', 11)]), ('misc', None)])\n"
     ]
    }
   ],
   "source": [
    "# Каждое предложение - список токенов\n",
    "# А токен представлен словарем значений\n",
    "for t in sentences[1][:10]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем корпус\n",
    "Давайте создадим список пар (tuple) слово (form) + тег (upostag) для каждого предложения. Получим структуру, аналогичную `tagged_sents` из `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Анкета', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "pos_sents = []\n",
    "for s in sentences:\n",
    "    # ВАШ КОД\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простейший POS-теггер\n",
    "Для каждого слова определим какими тегами оно бывает отмечено в обучающем корпусе, а для предсказания выбираем наиболее частотный тег:\n",
    "$$\n",
    "tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(tag_i \\mid w).\n",
    "$$\n",
    "NB! Если какое-то слово в обучающем корпусе мы не встретили, то тег на нем никаким образом уже поставить не сможем.\n",
    "\n",
    "Для этого нам нужно:\n",
    "* посчитать все частоты слово + тег:\n",
    "$$count(t_i, w_i)$$\n",
    "* частоту слова легко будет вывести из этих частот:\n",
    "$$count(w) = \\sum_{i=0...N}count(t_i, w_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать структуру `Counter` для хранения частот:\n",
    "`Counter[k] = 0` по умолчанию, поэтому можно сразу прибавлять:\n",
    "`Counter[k] += 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализатор для получения распределения вероятностей из частот\n",
    "# p(w) = count(w) / sum([count(w_i) for w_i in counter])\n",
    "def normalize(counter):\n",
    "    sum_ = sum(counter.values())\n",
    "    for token in counter:\n",
    "        counter[token] /= sum_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удобно использовать defaultdict, который заранее задает дефолтное значение для каждого ключа\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистическая модель\n",
    "# Здесь будут храниться слова и вероятности их тегов, например:\n",
    "# { 'знать': Counter({'NOUN': 0.3, 'VERB': 0.7})}\n",
    "class WordTagModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.model = defaultdict(Counter)\n",
    "        # Для каждого слова добавим Counter с частотами тегов\n",
    "        for sent in tagged_sents:\n",
    "            pass\n",
    "            # ВАШ КОД\n",
    "        # Не забудем нормализовать частоты, чтобы получить вероятности\n",
    "        for word in self.model:\n",
    "            pass\n",
    "            # ВАШ КОД\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        # специальная функция, чтобы получать значение по ключу (=слову)\n",
    "        return self.model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем теггер\n",
    "class SimplePOSTagger:\n",
    "    # Инициализировать теггер будем моделью word_tag_model, реализация которой выше\n",
    "    def __init__(self, word_tag_model):\n",
    "        self.wts = word_tag_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        # sent - предложение - список словоформ\n",
    "        tags = []\n",
    "        for word in sent:\n",
    "            # ВАШ КОД\n",
    "            # Что делать со словами, которых нет в модели?\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем\n",
    "wtm = WordTagModel(pos_sents)\n",
    "postagger = SimplePOSTagger(wtm)\n",
    "postagger.tag(\"Мама мыла раму\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос!**\n",
    "Как можно улучшить наш простой теггер?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM tagger\n",
    "Для каждого слова будем выбирать наиболее вероятный тег, учитывая общую вероятность самого тега. Тут можно будет добавить сглаживание для слов, которых в модели не было.\n",
    "$$\n",
    "     tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(w \\mid tag_i)P(tag_i)\n",
    "$$\n",
    "Для этого нам понадобятся новые классы:\n",
    "\n",
    "**EmissionModel**, хранящий для каждого тега вероятности быть присвоенным тому или иному слову:\n",
    "\n",
    "```\n",
    "defaultdict(\n",
    "  {\n",
    "    'tag_1': Counter({'word_1': 0.3, 'word_2': 0.7}), \n",
    "    'tag_2': Counter({'word_1': 0.6, 'word_3': 0.3 ...})\n",
    "  }\n",
    ")\n",
    "```\n",
    "\n",
    "**TransitionModel**, отвечающий за вероятность $P(tag_i)$:\n",
    "\n",
    "```\n",
    "Counter(\n",
    "  {\n",
    "    'tag_1': 0.1, \n",
    "    'tag_2': 0.33,\n",
    "    ...\n",
    "  }\n",
    ")\n",
    "```\n",
    "\n",
    "**UnigramPOSTagger**, сопоставляющий последовательности слов последовательность тегов, будет содержать `EmissionModel` и `TransitionModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionModel:\n",
    "    def __init__(self, tagged_sents):\n",
    "        self.model = defaultdict(Counter)\n",
    "        for sent in tagged_sents:\n",
    "            # ВАШ КОД\n",
    "            pass\n",
    "        self.add_unk_token()\n",
    "        for tag in self.model:\n",
    "            normalize(self.model[tag])\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        # Для каждого тега добавим одинаковую вероятность быть приписанным любому слову, которого нет в модели\n",
    "        for tag in self.model:\n",
    "            self.model[tag]['UNK'] = 0.01\n",
    "        \n",
    "    def tags(self):\n",
    "        # Добавим возможность возвращать все теги, которые есть в модели\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        # Все слова для данного тега\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, word, tag):\n",
    "        # Самое интересное - вероятность P(word|tag)\n",
    "        if word not in self[tag]:\n",
    "            return self[tag]['UNK']\n",
    "        return self[tag][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel:\n",
    "    def __init__(self, tagged_sents):\n",
    "        # Это модель будет хранить вероятности P(tag): Counter({'tag_1': 0.34, 'tag_2': 0.1 ...})\n",
    "        # Поэтому для неё нужны только пооследовательности тегов\n",
    "        self.model = Counter()\n",
    "        for sent in tagged_sents:\n",
    "            # ВАШ КОД\n",
    "            pass\n",
    "        normalize(self.model)\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, tag):\n",
    "        return self.model[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramPOSTagger:\n",
    "    def __init__(self, emission_model, transition_model):\n",
    "        self.em = emission_model\n",
    "        self.tm = transition_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        # Для списка слов возвращаем список пар (слово, тег)\n",
    "        # Для каждого слова проходимся по всем тегам\n",
    "        # И выбираем максимум по формуле\n",
    "        tags = []\n",
    "        for word in sent:\n",
    "            # ВАШ КОД\n",
    "            pass\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем!\n",
    "em = EmissionModel(pos_sents)\n",
    "tm = TransitionModel(pos_sents)\n",
    "unigram_postagger = UnigramPOSTagger(em, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_postagger.tag('Мама мыла раму'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-based tagger\n",
    "\n",
    "Довольно долго обучается в Google Colab:\n",
    "https://colab.research.google.com/drive/15KS4NtNKjzokzWYjIjaQ58MWWl4luZUr?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
