{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "NLP _NLTK_SYNTAX_blanked.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTY4l42d8GoG",
        "outputId": "d9b3eb22-115a-4a12-889a-987b1210d5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1SVhzHsD5r1"
      },
      "source": [
        "import itertools"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFurUsKyA92",
        "outputId": "16d484cf-e637-4650-a484-7551145a0cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download('all-corpora')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all-corpora'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all-corpora\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KteuSNvolccE"
      },
      "source": [
        "## Инструменты для работы в циклах (итераторы из Itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xqicII6ELdC"
      },
      "source": [
        "### zip и zip_longest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzb6rUXglhxu",
        "outputId": "4ad47da3-9030-4dbc-9326-0e2bbaa6943c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for a, b in zip ([1,2,3], [1,2,3]):\n",
        "  print (a+b)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNQAEcuvE1CE"
      },
      "source": [
        "Также с индексами (переюор массива циклом немножко быстрее, чем обращение по адрессу?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePrJ-i6_DSc-"
      },
      "source": [
        "про то, что могут быть неравными"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQjwUUXD2wO",
        "outputId": "f118c486-f64f-414e-e753-ec6415d14500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for t in itertools.zip_longest('ABCD', 'xy', fillvalue='-'):\n",
        "  print(t)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('A', 'x')\n",
            "('B', 'y')\n",
            "('C', '-')\n",
            "('D', '-')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDCrfBCaDDg_"
      },
      "source": [
        "zipped = zip([1,2,3], [1,2,3])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z87Oeos5DI1F",
        "outputId": "6593979a-557a-4294-abb4-87cd6e0ae24a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(zipped)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (2, 2), (3, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of2IfoQ7EUCi"
      },
      "source": [
        "### Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUQPepn9EjiA",
        "outputId": "2f206100-2cf1-4176-e840-c1faff0a6d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "for t in itertools.chain('ABCD', 'xy'):\n",
        "  print(t)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "x\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jbIvaxHEvzT",
        "outputId": "4469a799-1c6e-4c5a-bb2a-bbbf2fc5f870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "for t in itertools.chain.from_iterable(['ABCD', 'xy', 'we']):\n",
        "  print(t)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "x\n",
            "y\n",
            "w\n",
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-_eASQJFxZT"
      },
      "source": [
        "### Всякие фильтры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0T-D7r2F0ps",
        "outputId": "d732f958-ba94-4791-ed9c-d46f7c490fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for t in itertools.compress('ABCDEF', [1,0,1,0,1,1]):\n",
        "  print(t)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "C\n",
            "E\n",
            "F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnJVJRU-GDGp",
        "outputId": "0ca57f94-d938-45f6-8aa0-7c1870e913b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for t in itertools.dropwhile(lambda x: x<5, [1,4,6,4,1]):\n",
        "  print(t)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "4\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJaax5ncGPx4"
      },
      "source": [
        "Вы же знаете, что такое лямбда ?\n",
        "\n",
        "def lambda ( АРГУМЕНТЫ):\n",
        "\n",
        "return TRUE ИЛИ FALSE OТ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYNmqpvCGZJB",
        "outputId": "b3ebe4f6-8fc2-40e6-f875-e5d9d7c62286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "for t in itertools.takewhile(lambda x: x<5, [1,4,6,4,1]):\n",
        "  print(t)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuKp7GJ5GhMQ",
        "outputId": "e4f5259a-08f7-4226-a683-95528d891741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for t in itertools.filterfalse(lambda x: x%2, range(10)):\n",
        "  print(t)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMta6Bhk8GoR"
      },
      "source": [
        "## Синтаксис в NLTK\n",
        " #### Грамматика составляющих (помните такую?)\n",
        "\n",
        "\n",
        "NLTK умеет не только в морфу, но и в синтаксис. Помните, на самом первом занятии, когда мы занимались чанкингом (кстати, это що?), мы быстро начертили какое-то стрёмное дерево и побежали дальше?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fka8-pHOmNuq",
        "outputId": "7719fa47-dadb-4942-d172-276622433415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
        "\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(sentence)\n",
        "print(result)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT cat/NN))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkQ_AdcAmxVU"
      },
      "source": [
        "Так вот сегодня будем разбираться со всем этим подробнее"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mXhs4s68GoT"
      },
      "source": [
        "rules = \"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det ADJ N | Det N | NN\n",
        "    VP -> V NP \n",
        "    Det -> 'a'\n",
        "    ADJ -> 'tasty' | ADV ADJ\n",
        "    ADV -> 'very'\n",
        "    N -> 'fish' | 'fork' | 'dog' | 'boy'\n",
        "    NN -> 'Mary' | 'John'\n",
        "    V -> 'eats'\n",
        "\"\"\".split('\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6O-gXT6sLZJ"
      },
      "source": [
        "# сделали парсер\n",
        "grammar = nltk.CFG.fromstring('\\n'.join(rules))\n",
        "cp = nltk.EarleyChartParser(grammar)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J1fBtMf8GpO",
        "outputId": "fc7b98e9-bd2e-4718-a5ce-6def7c619eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def print_parses(parser, sentence):\n",
        "    for tree in parser.parse(sentence.split()):\n",
        "        print(tree)\n",
        "        \n",
        "print_parses(cp, \"Mary eats a fish\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP (NN Mary)) (VP (V eats) (NP (Det a) (N fish))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aPmo-F1srKv"
      },
      "source": [
        "Давайте посмотрим на иконичное предложение с 2 прочтениями I shot elephant in my pajamas. NLTK умеет выдавать сразу несколько деревьев, если вы ему дадите предложение с возможной неоднозначностью -- главное настроить так грамматику, чтобы он ловил эту неоднозначность и не ломался.\n",
        "\n",
        "Вывод ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoV0m5KSekqY",
        "outputId": "b99ab518-a07b-410a-8568-d14a2982e297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "\n",
        "\n",
        "el_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "\n",
        "# Your Code here\n",
        " \"\"\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-aaf423bb8e21>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    >>> el_grammar = nltk.CFG.fromstring(\"\"\"\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJ7JL9stvjL"
      },
      "source": [
        "Заметьте, что парсер поменялся -- там и в NLTK штук 10, но по факту отличаеются они мало."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbqxpIrwev-r",
        "outputId": "d92264ca-45ca-4068-9fbb-8aa3b9145adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        ">>> sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        ">>> parser = nltk.ChartParser(el_grammar) #заметьте, что у нас тут поменялся парсер\n",
        ">>> for tree in parser.parse(sent):\n",
        "...     print(tree)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-87f42bd35430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'an'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'elephant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'my'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pajamas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChartParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel_grammar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#заметьте, что у нас тут поменялся парсер\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'el_grammar' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dooZKWwIvQdz"
      },
      "source": [
        "Если Вам нужно обрабатывать большие тексты таким образом (и, соответственно, у вас там будет большая грамматика), то лучше создать текстовый файлик и брать правила из него. Расширение должно быть .cfg. You can then load it into NLTK and parse with it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe5lDZkjvsaH",
        "outputId": "836a8814-b63f-45bd-9717-eea0f8d0183b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        ">>> grammar1 = nltk.data.load('file:mygrammar.cfg') #у нас его нет\n",
        ">>> sent = \"Mary saw Bob\".split()\n",
        ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        ">>> for tree in rd_parser.parse(sent):\n",
        "...      print(tree)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-85ced5c30cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrammar1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file:mygrammar.cfg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#у нас его нет\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Mary saw Bob\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrd_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveDescentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcontent\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('content')\n  \u001b[0m\n  Searched in:\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LoDJDvfvoF2"
      },
      "source": [
        "When you write CFGs for parsing in NLTK, you cannot combine grammatical categories with lexical items on the righthand side of the same production. Thus, a production such as* PP -> 'of' NP* is disallowed. In addition, you are not permitted to place multi-word lexical items on the righthand side of a production. So rather than writing NP -> 'New\n",
        "York' *Курсив*, you have to resort to something like NP -> 'New_York' instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7Fa-wtv8Gp9"
      },
      "source": [
        "## Грамматики зависимостей ( а это что?)\n",
        "Можно использовать то же предложение, поскольку неоднозначность проявляется и в представлении зависимостей.\n",
        "В NLTK можно примерно одинаково работать с CFG и зависимостями (`DependencyGrammar`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPdyce_X8Gp-"
      },
      "source": [
        "dep_rules = \"\"\"\n",
        "... 'shot' -> 'I' | 'elephant' | 'in' | 'morning'\n",
        "... 'morning' -> 'One'\n",
        "... 'elephant' -> 'an' | 'in'\n",
        "... 'in' -> 'pajamas'\n",
        "... 'pajamas' -> 'my'\n",
        "... \"\"\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzkYE_ym8GqB"
      },
      "source": [
        "dep_grammar = nltk.DependencyGrammar.fromstring(dep_rules)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEvVDkHa8GqH",
        "outputId": "94cb56ff-ddb7-4ffc-8435-a7aa7e72efc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pdp = nltk.ProjectiveDependencyParser(dep_grammar)\n",
        "sent = 'One morning I shot an elephant in my pajamas'\n",
        "print_parses(pdp, sent)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(shot (morning One) I (elephant an (in (pajamas my))))\n",
            "(shot (morning One) I (elephant an) (in (pajamas my)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffel_vRQ8Gqe"
      },
      "source": [
        "## Встроенные корпуса и грамматики NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITv8fdjOMPBg"
      },
      "source": [
        "В NLTK лежат уже размеченные корпуса деревьев. Они нам потом пригодятся."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks3XmBzbxIKF",
        "outputId": "47b70e49-a34b-4024-d355-6e9edf398652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        ">>> from nltk.corpus import treebank\n",
        ">>> t = treebank.parsed_sents('wsj_0001.mrg')\n",
        ">>> print(t)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Tree('S', [Tree('NP-SBJ', [Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]), Tree(',', [',']), Tree('ADJP', [Tree('NP', [Tree('CD', ['61']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])]), Tree(',', [','])]), Tree('VP', [Tree('MD', ['will']), Tree('VP', [Tree('VB', ['join']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['board'])]), Tree('PP-CLR', [Tree('IN', ['as']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['nonexecutive']), Tree('NN', ['director'])])]), Tree('NP-TMP', [Tree('NNP', ['Nov.']), Tree('CD', ['29'])])])]), Tree('.', ['.'])]), Tree('S', [Tree('NP-SBJ', [Tree('NNP', ['Mr.']), Tree('NNP', ['Vinken'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP-PRD', [Tree('NP', [Tree('NN', ['chairman'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('NP', [Tree('NNP', ['Elsevier']), Tree('NNP', ['N.V.'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['the']), Tree('NNP', ['Dutch']), Tree('VBG', ['publishing']), Tree('NN', ['group'])])])])])]), Tree('.', ['.'])])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG66IoYvzO1o"
      },
      "source": [
        "Посмотрите на эту программу и скажите, что она делает?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaF3mNpWyGca"
      },
      "source": [
        "def filter(tree):\n",
        "    child_nodes = [child.label() for child in tree\n",
        "                   if isinstance(child, nltk.Tree)]\n",
        "    return  (tree.label() == 'VP') and ('S' in child_nodes)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApC0-6cpyNb_",
        "outputId": "e67e3274-5b82-4a42-9c86-52fd03244533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "for tr in t:\n",
        "  print(tr)\n",
        "  print(filter(tr))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP-SBJ\n",
            "    (NP (NNP Pierre) (NNP Vinken))\n",
            "    (, ,)\n",
            "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
            "    (, ,))\n",
            "  (VP\n",
            "    (MD will)\n",
            "    (VP\n",
            "      (VB join)\n",
            "      (NP (DT the) (NN board))\n",
            "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
            "      (NP-TMP (NNP Nov.) (CD 29))))\n",
            "  (. .))\n",
            "False\n",
            "(S\n",
            "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
            "  (VP\n",
            "    (VBZ is)\n",
            "    (NP-PRD\n",
            "      (NP (NN chairman))\n",
            "      (PP\n",
            "        (IN of)\n",
            "        (NP\n",
            "          (NP (NNP Elsevier) (NNP N.V.))\n",
            "          (, ,)\n",
            "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
            "  (. .))\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_KLeYOpzh8s"
      },
      "source": [
        "Окей, но что нам делать с неоднозначностью?\n",
        "\n",
        "Когда размер грамматики увеличивается, то пропорционально увеливается и количество разборов. Не редко NLTK дают ложные разборы (потому что относительно точную грамматику составить, конечно, можно, но сложно)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZiOzLK60RXq"
      },
      "source": [
        "Давайте возьмём иконичный (дебильный) пример из NLTK.\n",
        "\n",
        " Преложение fish fish fish fish fish, которое, кстати говоря, является абсолютно грамматическим английским предложением, неоднозначно в нашей грамматике. Как автоматически понять, какой разбор лучше?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tkk7V5Jz-hq",
        "outputId": "4f792ae5-173e-4b46-d01a-b4b8b8aaab6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        ">>> grammar = nltk.CFG.fromstring(\"\"\"\n",
        "... S -> NP V NP\n",
        "... NP -> NP Sbar\n",
        "... Sbar -> NP V\n",
        "... NP -> 'fish'\n",
        "... V -> 'fish'\n",
        "... \"\"\")\n",
        "\n",
        "# 'fish that other fish fish are in the habit of fishing fish themselves'.\n",
        ">>> tokens = [\"fish\"] * 5\n",
        ">>> cp = nltk.ChartParser(grammar)\n",
        ">>> for tree in cp.parse(tokens):\n",
        "...     print(tree)\n",
        "# ну и какое верное, кстати говоря?"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\n",
            "(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cawbvIN16Fjq"
      },
      "source": [
        "какие есть решения?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXoE2Er-6IOu"
      },
      "source": [
        "1. Особые классы словосочетаний"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFf8gxE8zbPw"
      },
      "source": [
        "rule_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "VPpred -> Vpred Adj\n",
        "VPn -> Vn NP\t\n",
        "VPs -> Vs S\t\n",
        "VPdat -> Vdat Np NP\n",
        "VPphrase -> Vphrase NP PP\t\n",
        "Vpred -> 'was'\n",
        "Vn -> 'saw'\n",
        "Vs -> 'thought'\n",
        "Vphrase -> 'put'\n",
        "Vdat -> 'give'\n",
        " \"\"\")\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcFmSrA65FG"
      },
      "source": [
        "2. Weighted grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpSc-tni7Pcu"
      },
      "source": [
        "У нас есть большой корпус деревьев в NLTK. Можно посчитать вероятность каждой конструкции. Чуть выше мы видели код, который найдет все Vp -> V S. Можно такие вычисления проделать для всего и добавить это в нашу грамматику в таком формате."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olnVkagL61aW"
      },
      "source": [
        "\t\n",
        "grammar = nltk.PCFG.fromstring(\"\"\"\n",
        "    S    -> NP VP              [1.0]\n",
        "    VP   -> TV NP              [0.4]\n",
        "    VP   -> IV                 [0.3]\n",
        "    VP   -> DatV NP NP         [0.3]\n",
        "    TV   -> 'saw'              [1.0]\n",
        "    IV   -> 'ate'              [1.0]\n",
        "    DatV -> 'gave'             [1.0]\n",
        "    NP   -> 'telescopes'       [0.8]\n",
        "    NP   -> 'Jack'             [0.2]\n",
        "    \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9KYnK9f76Dq",
        "outputId": "70f239c2-1b5d-45da-c560-45555b2389a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ">>> viterbi_parser = nltk.ViterbiParser(grammar) #снова новый парсер\n",
        ">>> for tree in viterbi_parser.parse(['Jack', 'saw', 'telescopes']):\n",
        "...     print(tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvCxYgJ59DBm"
      },
      "source": [
        "3. Морфология"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wxvt2m1BMOw",
        "outputId": "defbcd99-f989-4248-f20f-2548765e6f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\n",
        "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\n",
        "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\n",
        "\n",
        "V[SUBCAT=intrans, TENSE=pres, NUM=sg] -> 'disappears' | 'walks'\n",
        "V[SUBCAT=trans, TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
        "V[SUBCAT=clause, TENSE=pres, NUM=sg] -> 'says' | 'claims'\n",
        "\n",
        "V[SUBCAT=intrans, TENSE=pres, NUM=pl] -> 'disappear' | 'walk'\n",
        "V[SUBCAT=trans, TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
        "V[SUBCAT=clause, TENSE=pres, NUM=pl] -> 'say' | 'claim'\n",
        "\n",
        "V[SUBCAT=intrans, TENSE=past, NUM=?n] -> 'disappeared' | 'walked'\n",
        "V[SUBCAT=trans, TENSE=past, NUM=?n] -> 'saw' | 'liked'\n",
        "V[SUBCAT=clause, TENSE=past, NUM=?n] -> 'said' | 'claimed'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-c6838c330fbd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGFr3Z1CBReQ"
      },
      "source": [
        "4. Всё вместе"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHLlLT98OGg"
      },
      "source": [
        "Задание 1.\n",
        "\n",
        "Любым доступным способом сделайте дерево для  Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo. Посмотрите на дерево по ссылке и напишите грамматику, чтобы получилось также и был только один вариант  http://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo. \n",
        "\n",
        "(можно сделать без применения средств дисамбигуации)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrrKWH5h9AIu"
      },
      "source": [
        "# Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryf4fMIBmIx"
      },
      "source": [
        "## Разумеется, всё у нас уже готово в NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57bUg9oYBxFU",
        "outputId": "5d62f7fb-b3f5-496a-f5e0-6362813f7f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "nltk.download ('book_grammars')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package book_grammars to /root/nltk_data...\n",
            "[nltk_data]   Unzipping grammars/book_grammars.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yheQLOEqBs1-",
        "outputId": "cce8ae13-381d-4e45-f56e-6287881c513d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% start S\n",
            "# ###################\n",
            "# Grammar Productions\n",
            "# ###################\n",
            "S[-INV] -> NP VP\n",
            "S[-INV]/?x -> NP VP/?x\n",
            "S[-INV] -> NP S/NP\n",
            "S[-INV] -> Adv[+NEG] S[+INV]\n",
            "S[+INV] -> V[+AUX] NP VP\n",
            "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
            "SBar -> Comp S[-INV]\n",
            "SBar/?x -> Comp S[-INV]/?x\n",
            "VP -> V[SUBCAT=intrans, -AUX]\n",
            "VP -> V[SUBCAT=trans, -AUX] NP\n",
            "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
            "VP -> V[SUBCAT=clause, -AUX] SBar\n",
            "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
            "VP -> V[+AUX] VP\n",
            "VP/?x -> V[+AUX] VP/?x\n",
            "# ###################\n",
            "# Lexical Productions\n",
            "# ###################\n",
            "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
            "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
            "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
            "V[+AUX] -> 'do' | 'can'\n",
            "NP[-WH] -> 'you' | 'cats'\n",
            "NP[+WH] -> 'who'\n",
            "Adv[+NEG] -> 'rarely' | 'never'\n",
            "NP/NP ->\n",
            "Comp -> 'that'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc5GR-ZoB9BO",
        "outputId": "ab08084b-a703-4f4c-9556-eea4bed789ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        ">>> tokens = 'who do you claim that you like'.split()\n",
        ">>> from nltk import load_parser\n",
        ">>> cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
        ">>> for tree in cp.parse(tokens):\n",
        "...     print(tree)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S[-INV]\n",
            "  (NP[+WH] who)\n",
            "  (S[+INV]/NP[]\n",
            "    (V[+AUX] do)\n",
            "    (NP[-WH] you)\n",
            "    (VP[]/NP[]\n",
            "      (V[-AUX, SUBCAT='clause'] claim)\n",
            "      (SBar[]/NP[]\n",
            "        (Comp[] that)\n",
            "        (S[-INV]/NP[]\n",
            "          (NP[-WH] you)\n",
            "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha0J2vos8GrQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}