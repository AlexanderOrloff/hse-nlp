{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка\n",
    "\n",
    "Это предварительные действия, которые лучше выполнить перед семинаром!\n",
    "\n",
    "1. Скачать файл с моделью AdaGram для русского языка по [ссылке](https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib)\n",
    "2. Переместить файл из п. 1 в папку с этой тетрадкой\n",
    "3. Скачать архив с моделью ELMo для русского языка по [ссылке](http://vectors.nlpl.eu/repository/20/196.zip)\n",
    "4. Распаковать архив в папку с этой тетрадкой\n",
    "5. Скачать в папку с этой тетрадкой дополнительные файлы [из репозитория курса](5_WSD)\n",
    "6. Установить необходимые библиотеки (ячейки ниже)\n",
    "\n",
    "P.S. Можно проделать всё это в Colab'е, но там есть проблемы с запуском MyStem :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow Cython matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simple-elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/lopuhin/python-adagram.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymystem3 pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# библиотеки для работы с эмбеддингами\n",
    "import adagram\n",
    "from simple_elmo import ElmoModel\n",
    "\n",
    "# обработка данных и ML\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import *\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "token = RegexpTokenizer('\\w+')\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    words = [morph.parse(word)[0].normal_form for word in tokenize(text) if word]\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    return token.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Адаграм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec и многие другие векторные модели сопоставляют 1 вектор. Это значит, что у каждого слова в векторном пространстве только 1 значение. У многозначных слов векторы будут просто каким-то усреднением или обобщением всех его значений. \n",
    "\n",
    "В работе https://arxiv.org/pdf/1502.07257.pdf предлагается способ улучшить Skip Gram, так чтобы каждому слову сопоставлялось K различных векторов, так что каждый из них представляет какое-то из его значений. При этом сам параметр K задавать не нужно, модель сама находит нужное количество \"значений\" для каждого слова.\n",
    "\n",
    "Изначально этот  подход реализован на julia, но есть реализация на питоне - https://github.com/lopuhin/python-adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl \"https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib\" > all.a010.p10.d300.w5.m100.nonorm.slim.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на значения каких-нибудь слов: эти \"значения\"  задаются индексами, у каждого есть вероятность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.word_sense_probs('вечер')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие слова близки к каждому из значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Сегодня вечером я иду в гости.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВОПРОС! Как можно дизамбигуировать контексты, используя соседей для каждого значения?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на все слова у которых есть хотя бы 2 устойчивых значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous = []\n",
    "for i, word in enumerate(vm.dictionary.id2word):\n",
    "    probs = vm.word_sense_probs(word)\n",
    "    if len(probs) > 1:\n",
    "        ambiguous.append(word)\n",
    "print(ambiguous[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дизамбигуация AdaGram основана на вычислении вероятности вектора каждого значения в заданном контексте. \n",
    "\n",
    "Функция `model.disambiguate` возвращает массив вероятностей для всех значения данного слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = vm.disambiguate('вечер', normalize(\"Ради любви родителей, ради того, чтобы они снова также танцевали в их гостиной, наслаждаясь милыми семейными\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы выяснить, какое значение выбрал AdaGram, нужно найти индекс вектора с максимальной вероятностью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', np.argmax(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = vm.disambiguate('вечер', normalize(\"абонемент № 19 \\\"Камерные\\\" включает в себя и концерт лауреата последнего Конкурса Чайковского\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', np.argmax(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSD / WSI\n",
    "Разрешение семантической/лексической неоднозначности/омонимии\n",
    "\n",
    "Проверим, насколько хорошо выбирается значение на данных с [соревнования Диалога](http://www.dialog-21.ru/evaluation/2018/disambiguation/) (переиспользую [baseline](https://github.com/nlpub/russe-wsi-kit) соревнования)\n",
    "(А [вот](http://www.dialog-21.ru/media/5077/bolshinaasplusloukachevitchnv-108.pdf), кстати, новая статья о генерации обучающих данных для WSD)\n",
    "\n",
    "**NB!** Большая модель AdaGram для русского языка, которую мы используем, обучена на корпусе с нормализацией *mystem*. Так что немного модифицируем нашу функцию нормализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "def lemmatized_context(s):\n",
    "    return [w.lower() for w in mystem.lemmatize(\" \".join(tokenize(s)))]\n",
    "\n",
    "def disambiguate(model, word, context):\n",
    "    word, _ = lemmatized_context(word)\n",
    "    probs = model.disambiguate(word, lemmatized_context(context))\n",
    "    return 1 + probs.argmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшой подкорпус RUSSE - всего 4 неоднозначных слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_WSD/train.baseline-adagram.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(frac=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['predict_sense_id'] = [disambiguate(vm, word, context)\n",
    "                          for word, context in tqdm(zip(df['word'], df['context']), total=len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('word').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_word = df.groupby('word').apply(\n",
    "    lambda f: adjusted_rand_score(f['gold_sense_id'], f['predict_sense_id'])\n",
    ").to_frame('ARI')\n",
    "per_word_ari = per_word['ARI']\n",
    "print('Mean word ARI: %.4f' % np.mean(per_word_ari))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики используется [Adjuster Rand Index](https://en.wikipedia.org/wiki/Rand_index), а [вот ссылка на документацию](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized embeddings\n",
    "[ELMo](https://arxiv.org/pdf/1802.05365.pdf) — модель, которая позволяет получить контекстуальный (contextualized) вектор слова  - учитывающий контекст:\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/Bert-language-modeling.png\" alt=\"elmo\" width=\"400\"/>\n",
    "\n",
    "(Подробнее в слайдах лекции)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С ELMo легко работать с помощью библиотеки simple_elmo.\n",
    "Скачиваем модель [отсюда](http://vectors.nlpl.eu/repository/20/196.zip) (см. инструкции выше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElmoModel()\n",
    "model.load(\"196\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У модели есть метод `get_elmo_vectors`, который принимает на вход массив контекстов - и возвращает массив матриц векторов - для каждого слова каждого входного текста.\n",
    "Нормализуем предложение и достанем контекстуализированный вектор неоднозначного слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"многочисленные укрепленные монастыри также не являлись замками как таковыми — это были крепости\"\n",
    "tokens = normalize(sentence)\n",
    "word_idx = tokens.index(\"замок\")\n",
    "word_vector = model.get_elmo_vectors([tokens])[0][word_idx]  # 0 - индекс контекста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы каждый раз не повторять эту процедуру, обернём в свою функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_vectors(word, contexts, model):\n",
    "    tokens = [normalize(c) for c in contexts]\n",
    "    all_vectors = model.get_elmo_vectors(tokens)\n",
    "    word_vecs = []\n",
    "    for i in range(len(contexts)):\n",
    "        try:\n",
    "            word_vecs.append(all_vectors[i][tokens[i].index(word)])\n",
    "        except ValueError:  # если нормализация накосячила и лемму не найти\n",
    "            continue\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сначала нарисовать, какие получаются вектора одного и того же слова в разных контекстах (пропустим немного заранее заготовленной магии matplotlib и PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction(X, n):\n",
    "    pca = PCA(n_components=n)\n",
    "    print(\"size of X: {}\".format(X.shape))\n",
    "    results = pca.fit_transform(X)\n",
    "    print(\"size of reduced X: {}\".format(results.shape))\n",
    "\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(\"Variance retained ratio of PCA-{}: {}\".format(i+1, ratio))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(word, contexts, labels, reduced_X, context_size=5):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 10)\n",
    "    colors = ['ro', 'bo', 'yo', 'go', 'co']\n",
    "    label_color = {}\n",
    "    for i, l in enumerate(set(labels)):\n",
    "        label_color[l] = colors[i]\n",
    "\n",
    "    i = 0\n",
    "    points = []\n",
    "    tokens_list = []\n",
    "    for j, (c, l) in enumerate(zip(contexts, labels)):\n",
    "        tokens = normalize(c)\n",
    "        tokens_list.append(tokens)\n",
    "        color = label_color[l[0]]\n",
    "        for k, w in enumerate(tokens):\n",
    "            if w == word:  # рисуем первое вхождение слова в контексте\n",
    "                ax.plot(reduced_X[j, 0], reduced_X[j, 1], color)\n",
    "                points.append((j, k, reduced_X[j, 0], reduced_X[j, 1]))\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    for p in points:\n",
    "        s = tokens_list[p[0]]\n",
    "        text = ' '.join(s[max(0, p[1] - context_size):min(p[1] + context_size, len(s))])\n",
    "\n",
    "        # bold the word of interest in the sentence\n",
    "        text = text.replace(word, r\"$\\bf{\" + word + \"}$\")\n",
    "\n",
    "        plt.annotate(text, xy=p[2:])\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И возьмем датасет побольше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('5_WSD/train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все слова датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(train['word'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['word'] == 'замок']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = train[train['word']=='замок'][train['gold_sense_id']=='1'].sample(5, random_state=21)\n",
    "sentences_2 = train[train['word']=='замок'][train['gold_sense_id']=='2'].sample(5, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(sentences_1['context']) + list(sentences_2['context'])\n",
    "labels = list(sentences_1['gold_sense_id']) + list(sentences_2['gold_sense_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_elmo_vectors('замок', sentences, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduce = dim_reduction(X=X, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('замок', sentences, labels, X_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно сделать с этими векторами в целях WSD?\n",
    "* классификация\n",
    "* кластеризация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (в группах)\n",
    "\n",
    "Вариант 1:\n",
    "\n",
    "- Напишите функцию, которая вычисляет центроид (средний вектор) каждого значения данного слова по всем контекстам из `train`.\n",
    "- Напишите вторую функцию, которая принимает на вход слово и произвольный контекст с этим словом, а возвращает индекс значения слова в этом контексте: вычисляем контекстный вектор, сравниваем с центроидами значений, выбираем ближайшее. Можно также вывести насколько контекстов для этого значения из обучающего множества.\n",
    "\n",
    "Вариант 2:\n",
    "- Выберите один их методов кластеризации:\n",
    "  - [K-Means]()\n",
    "  - [Affinity Propagation]()\n",
    "  - или любой другой приятный вам метод из [sklearn.cluster]()\n",
    "- Напишите функцию, которая будет принимать на вход слово, кластеризовать его контексты из `train` и вычислять ARI по сравнению с эталонной разметкой значений.\n",
    "- (*) Если останется время, можно нарисовать получившуюся кластеризацию - с помощью функции `plot`, которая определена выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
