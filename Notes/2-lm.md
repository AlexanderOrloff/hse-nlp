## Statistical language models
!!важно!! [Jurafsky+Martin Chapter 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf)

более подробно про сглаживание: [Michael Collins' NLP course slides](http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf)

[EM-алгоритм "в общих чертах"](https://habr.com/ru/post/501850/)

[G. Neubig. Neural machine translation and sequence-to-sequence models](https://arxiv.org/pdf/1703.01619.pdf) - разделы 3-4

## Word embeddings

[Sebastian Ruder's post on word embeddings](https://ruder.io/word-embeddings-1/index.html)
[Word2Vec tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

## Neural language models
[G. Neubig. Neural machine translation and sequence-to-sequence models](https://arxiv.org/pdf/1703.01619.pdf) - разделы 5-6

[Karlsruhe Machine Translation course](https://www.coursera.org/lecture/machinetranslation/feed-foward-neural-network-language-model-2NSTS) - лекция про нейронные языковые модели с картинками

Ещё туториал с картинками: [часть 1 - forward pass](https://medium.com/@SauceCat/forward-propagation-for-feed-forward-networks-ac8fcb6bdd60), [часть 2 - backward pass](https://towardsdatascience.com/backward-propagation-for-feed-forward-networks-afdf9d038d21)

В NLP in action (см. материалы к 1 лекции) - Глава 5, **Baby steps with neural networks**
